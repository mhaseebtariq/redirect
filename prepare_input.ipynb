{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19cb5afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.sql import types as st\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import settings as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd40b21f-0f55-40dc-93eb-e449cd1cf5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major, \n",
    "    sys.version_info.minor, \n",
    "    sys.version_info.micro,\n",
    ") != (3, 9, 19):\n",
    "    raise EnvironmentError(\"Only runs efficiently on Python 3.9.19 | conda 24.1.2 | Apple M3 Pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b85caee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/envs/redirect-39/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/haseeb.tariq/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/haseeb.tariq/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-93bd8962-362a-4288-9f21-6d6187c1764f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.3-spark3.5-s_2.13 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 67ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.3-spark3.5-s_2.13 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-93bd8962-362a-4288-9f21-6d6187c1764f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/2ms)\n",
      "24/06/23 17:07:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "config = [\n",
    "    (\"spark.jars.packages\", \"graphframes:graphframes:0.8.3-spark3.5-s_2.13\"),\n",
    "    (\"spark.driver.memory\", \"8g\"),\n",
    "    (\"spark.worker.memory\", \"8g\"),\n",
    "]\n",
    "spark = SparkSession.builder.appName(\"testing\").config(conf=SparkConf().setAll(config)).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a2d4cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(value):\n",
    "    return f\"id-{hash(value)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa090168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000000\n",
      "40000000\n",
      "60000000\n",
      "80000000\n",
      "100000000\n",
      "120000000\n",
      "140000000\n",
      "160000000\n",
      "CPU times: user 1min 14s, sys: 6.52 s, total: 1min 21s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    os.remove(s.STAGED_DATA_CSV_LOCATION)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "with open(s.DATA_FILE) as in_file:\n",
    "    cnt = 0\n",
    "    lines = \"\"\n",
    "    for line in in_file:\n",
    "        cnt += 1\n",
    "        if cnt == 1:\n",
    "            continue\n",
    "        line = line.strip()\n",
    "        lines += f\"{get_id(line)},{line}\\n\"\n",
    "        if not (cnt % 2e7):\n",
    "            print(cnt)\n",
    "            with open(s.STAGED_DATA_CSV_LOCATION, \"a\") as out_file:\n",
    "                out_file.write(lines)\n",
    "                lines = \"\"\n",
    "if lines:\n",
    "    lines = lines.strip()\n",
    "    with open(s.STAGED_DATA_CSV_LOCATION, \"a\") as out_file:\n",
    "        out_file.write(lines)\n",
    "        del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "467df787",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove(s.STAGED_PATTERNS_CSV_LOCATION)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "lines = \"\"\n",
    "with open(s.PATTERNS_FILE) as in_file:\n",
    "    for line in in_file:\n",
    "        line = line.strip()\n",
    "        if line[:4].isnumeric():\n",
    "            lines += f\"{get_id(line)},{line}\\n\"\n",
    "        else:\n",
    "            lines += f\"{line}\\n\"\n",
    "\n",
    "lines = lines.strip()\n",
    "with open(s.STAGED_PATTERNS_CSV_LOCATION, \"a\") as out_file:\n",
    "    out_file.write(lines)\n",
    "    del lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd1cf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = st.StructType(\n",
    "    [\n",
    "        st.StructField(\"transaction_id\", st.StringType(), False),\n",
    "        st.StructField(\"timestamp\", st.TimestampType(), False),\n",
    "        st.StructField(\"source_bank\", st.StringType(), False),\n",
    "        st.StructField(\"source\", st.StringType(), False),\n",
    "        st.StructField(\"target_bank\", st.StringType(), False),\n",
    "        st.StructField(\"target\", st.StringType(), False),\n",
    "        st.StructField(\"received_amount\", st.FloatType(), False),\n",
    "        st.StructField(\"receiving_currency\", st.StringType(), False),\n",
    "        st.StructField(\"sent_amount\", st.FloatType(), False),\n",
    "        st.StructField(\"sending_currency\", st.StringType(), False),\n",
    "        st.StructField(\"format\", st.StringType(), False),\n",
    "        st.StructField(\"is_laundering\", st.IntegerType(), False),\n",
    "    ]\n",
    ")\n",
    "columns = [x.name for x in schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2ee4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(s.STAGED_PATTERNS_CSV_LOCATION, \"r\") as fl:\n",
    "    patterns = fl.read()\n",
    "\n",
    "cases = []\n",
    "case_id = 0\n",
    "for pattern in patterns.split(\"\\n\\n\"):\n",
    "    case_id += 1\n",
    "    if not pattern.strip():\n",
    "        continue\n",
    "    pattern = pattern.split(\"\\n\")\n",
    "    name = pattern.pop(0).split(\" - \")[1]\n",
    "    category, sub_category = name, name\n",
    "    if \": \" in name:\n",
    "        category, sub_category = name.split(\": \")\n",
    "    pattern.pop()\n",
    "    case = pd.DataFrame([x.split(\",\") for x in pattern], columns=columns)\n",
    "    case.loc[:, \"id\"] = case_id\n",
    "    case.loc[:, \"type\"] = category\n",
    "    case.loc[:, \"sub_type\"] = sub_category\n",
    "    cases.append(case)\n",
    "cases = pd.concat(cases, ignore_index=True)\n",
    "cases = spark.createDataFrame(cases)\n",
    "cases = cases.withColumn(\"timestamp\", sf.to_timestamp(\"timestamp\", s.TIMESTAMP_FORMAT))\n",
    "cases = cases.select(\n",
    "    \"transaction_id\", \"id\", \"type\", \"sub_type\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45180d75-a337-468b-89e3-38f1b82646f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENCY_MAPPING = {\n",
    "    \"Australian Dollar\": \"aud\",\n",
    "    \"Bitcoin\": \"btc\",\n",
    "    \"Brazil Real\": \"brl\",\n",
    "    \"Canadian Dollar\": \"cad\",\n",
    "    \"Euro\": \"eur\",\n",
    "    \"Mexican Peso\": \"mxn\",\n",
    "    \"Ruble\": \"rub\",\n",
    "    \"Rupee\": \"inr\",\n",
    "    \"Saudi Riyal\": \"sar\",\n",
    "    \"Shekel\": \"ils\",\n",
    "    \"Swiss Franc\": \"chf\",\n",
    "    \"UK Pound\": \"gbp\",\n",
    "    \"US Dollar\": \"usd\",\n",
    "    \"Yen\": \"jpy\",\n",
    "    \"Yuan\": \"cny\"\n",
    "}\n",
    "\n",
    "currency_code = sf.udf(lambda x: CURRENCY_MAPPING[x], st.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edb394c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/23 17:11:15 WARN TaskSetManager: Stage 1 contains a task of very large size (1849 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 11:==================================================>   (119 + 9) / 128]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 211 ms, sys: 74 ms, total: 285 ms\n",
      "Wall time: 6min 38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "179504480"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data = spark.read.csv(\n",
    "    s.STAGED_DATA_CSV_LOCATION, header=False, schema=schema, timestampFormat=s.TIMESTAMP_FORMAT\n",
    ")\n",
    "group_by = [\n",
    "    \"timestamp\",\n",
    "    \"source_bank\",\n",
    "    \"source\",\n",
    "    \"target_bank\",\n",
    "    \"target\",\n",
    "    \"receiving_currency\",\n",
    "    \"sending_currency\",\n",
    "    \"format\",\n",
    " ]\n",
    "data = data.groupby(group_by).agg(\n",
    "    sf.first(\"transaction_id\").alias(\"transaction_id\"),\n",
    "    sf.collect_set(\"transaction_id\").alias(\"transaction_ids\"),\n",
    "    sf.sum(\"received_amount\").alias(\"received_amount\"), \n",
    "    sf.sum(\"sent_amount\").alias(\"sent_amount\"),\n",
    "    sf.max(\"is_laundering\").alias(\"is_laundering\"),\n",
    ")\n",
    "data = data.withColumn(\n",
    "    \"source_currency\", currency_code(sf.col(\"sending_currency\"))\n",
    ").withColumn(\n",
    "    \"target_currency\", currency_code(sf.col(\"receiving_currency\")),\n",
    ")\n",
    "data = data.join(cases, on=\"transaction_id\", how=\"left\").repartition(128, \"transaction_id\")\n",
    "data = data.select(\n",
    "    \"transaction_id\",\n",
    "    \"transaction_ids\",\n",
    "    \"timestamp\",\n",
    "    sf.concat(sf.col(\"source\"), sf.lit(\"-\"), sf.col(\"source_currency\")).alias(\"source\"),\n",
    "    sf.concat(sf.col(\"target\"), sf.lit(\"-\"), sf.col(\"target_currency\")).alias(\"target\"),\n",
    "    \"source_bank\",\n",
    "    \"target_bank\",\n",
    "    \"source_currency\",\n",
    "    \"target_currency\",\n",
    "    sf.col(\"sent_amount\").alias(\"source_amount\"),\n",
    "    sf.col(\"received_amount\").alias(\"target_amount\"),\n",
    "    \"format\",\n",
    "    \"is_laundering\",\n",
    ").persist(StorageLevel.DISK_ONLY)\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72d4b7c6-8740-48ac-9314-25ebf3e56862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "197599"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.select(sf.explode(\"transaction_ids\")).count() - data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0364dec-3ed8-4213-b7a8-4f6f9eba67dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/23 17:15:51 WARN TaskSetManager: Stage 30 contains a task of very large size (1849 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cases_data = cases.join(\n",
    "    data.withColumnRenamed(\"transaction_id\", \"x\").drop(*cases.columns).select(\n",
    "        sf.explode(\"transaction_ids\").alias(\"transaction_id\"), \"*\"\n",
    "    ),\n",
    "    on=\"transaction_id\",\n",
    "    how=\"left\",\n",
    ").drop(\"is_laundering\", \"transaction_id\", \"transaction_ids\").withColumnRenamed(\"x\", \"transaction_id\")\n",
    "cases_data.toPandas().to_parquet(s.STAGED_CASES_DATA_LOCATION)\n",
    "cases_data = pd.read_parquet(s.STAGED_CASES_DATA_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12c5cfc5-cc6f-45b0-b717-bb73c52e9df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = data.drop(\"transaction_ids\")\n",
    "data.write.parquet(s.STAGED_DATA_LOCATION, mode=\"overwrite\")\n",
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15609486-de3c-4470-928e-612aa35a08e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "assert data.count() == data.select(\"transaction_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4276821f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179504480, 137936, 137933, 16467)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count(), cases_data.shape[0], cases_data[\"transaction_id\"].nunique(), cases_data[\"id\"].nunique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
