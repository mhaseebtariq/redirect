{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd66cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import timedelta, date\n",
    "from glob import glob\n",
    "\n",
    "import leidenalg as la\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from pyspark.sql import functions as sf, types as st\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import settings as s\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78718f0-7f15-4c1f-963f-bd160efec0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major, \n",
    "    sys.version_info.minor, \n",
    "    sys.version_info.micro,\n",
    ") != (3, 11, 7):\n",
    "    raise EnvironmentError(\"Only runs efficiently on Python 3.11.7 | conda 24.1.2 | Apple M3 Pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff6f299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/anaconda3/envs/redirect/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/haseeb.tariq/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/haseeb.tariq/.ivy2/jars\n",
      "graphframes#graphframes added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e5f386aa-6e17-4a8d-9ada-255ddca9ced2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.8.3-spark3.5-s_2.13 in spark-packages\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      ":: resolution report :: resolve 60ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tgraphframes#graphframes;0.8.3-spark3.5-s_2.13 from spark-packages in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e5f386aa-6e17-4a8d-9ada-255ddca9ced2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/2ms)\n",
      "24/06/23 23:14:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/23 23:14:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "config = [\n",
    "    (\"spark.jars.packages\", \"graphframes:graphframes:0.8.3-spark3.5-s_2.13\"),\n",
    "    (\"spark.driver.memory\", \"8g\"),\n",
    "    (\"spark.worker.memory\", \"8g\"),\n",
    "]\n",
    "spark = SparkSession.builder.appName(\"testing\").config(conf=SparkConf().setAll(config)).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdfc52ce-1f9d-49d2-81f7-366fcdab6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_script = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2442c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)\n",
    "data = data.where(sf.col(\"source\") != sf.col(\"target\"))\n",
    "data = data.where(sf.col(\"format\").isin([\"ACH\", \"Wire\", \"Bitcoin\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f2f687f-1bdd-4995-9fc7-cd17e58c7c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_edges(data_input):\n",
    "    data_aggregated = data_input.groupby([\"source\", \"target\"]).agg(\n",
    "        sf.sum(\"source_amount\").alias(\"source_amount\"),\n",
    "        sf.sum(\"target_amount\").alias(\"target_amount\"),\n",
    "    ).toPandas()\n",
    "    \n",
    "    source_totals = data_aggregated.groupby(\n",
    "        \"source\"\n",
    "    ).agg({\"source_amount\": \"sum\"})[\"source_amount\"].to_dict()\n",
    "    target_totals = data_aggregated.groupby(\n",
    "        \"target\"\n",
    "    ).agg({\"target_amount\": \"sum\"})[\"target_amount\"].to_dict()\n",
    "    \n",
    "    data_aggregated.loc[:, \"total_sent_by_source\"] = data_aggregated.loc[:, \"source\"].apply(\n",
    "        lambda x: source_totals[x]\n",
    "    )\n",
    "    data_aggregated.loc[:, \"total_received_by_target\"] = data_aggregated.loc[:, \"target\"].apply(\n",
    "        lambda x: target_totals[x]\n",
    "    )\n",
    "    data_aggregated.loc[:, \"weight\"] = data_aggregated.apply(\n",
    "        lambda x: (\n",
    "            (x[\"source_amount\"] / x[\"total_sent_by_source\"]) +\n",
    "            (x[\"target_amount\"] / x[\"total_received_by_target\"])\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    data_aggregated.loc[:, \"source\"] = data_aggregated[\"source\"].str.slice(0, 8)\n",
    "    data_aggregated.loc[:, \"target\"] = data_aggregated[\"target\"].str.slice(0, 8)\n",
    "    filter_self = data_aggregated[\"source\"] != data_aggregated[\"target\"]\n",
    "    data_aggregated = data_aggregated.loc[filter_self, :].reset_index(drop=True)\n",
    "    return data_aggregated.loc[:, [\"source\", \"target\", \"weight\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17ce13b4-afe0-41d6-bfb5-c558d635ec4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reversed_graph(edges_data):\n",
    "    columns = [\"source\", \"target\", \"weight\"]\n",
    "    edges_r = edges_data.loc[:, columns].rename(\n",
    "        columns={\"target\": \"source\", \"source\": \"target\"}\n",
    "    ).copy(deep=True).loc[:, columns]\n",
    "    return ig.Graph.DataFrame(edges_r, use_vids=False, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bcb53cf-ffc6-42ad-bc74-87b5f8cc2c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.7 s, sys: 503 ms, total: 29.2 s\n",
      "Wall time: 42.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "edges = aggregate_edges(data)\n",
    "graph = ig.Graph.DataFrame(edges, use_vids=False, directed=True)\n",
    "nodes = [x[\"name\"] for x in graph.vs()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bba166fa-7b4f-4e05-b3cd-8aac40f5a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processes(ids):\n",
    "    processes = []\n",
    "    for process in psutil.process_iter():\n",
    "        cmdline = []\n",
    "        try:\n",
    "            cmdline = process.cmdline()\n",
    "        except Exception as error:\n",
    "            pass\n",
    "        if ids.intersection(cmdline):\n",
    "            processes.append(process)\n",
    "    return processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c20b900-1cff-4b99-83cf-9072ed4ac5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUMBER_OF_PROCESSES = 10\n",
    "\n",
    "shutil.rmtree(\"staging\", ignore_errors=True)\n",
    "os.mkdir(\"staging\")\n",
    "chunks = np.array_split(nodes, NUMBER_OF_PROCESSES)\n",
    "\n",
    "filename = \"graph.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(graph, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "filename = \"nodes.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(chunks, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "process_ids = set()\n",
    "process_name = \"communities.py\"\n",
    "for chunk_number in range(NUMBER_OF_PROCESSES):\n",
    "    process_id = str(uuid.uuid4())\n",
    "    process_ids = process_ids.union({process_id})\n",
    "    os.system(f\"{sys.executable} {process_name} {chunk_number} {process_id} &\")\n",
    "\n",
    "while get_processes(process_ids):\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0293b2-a6f7-4d28-b1df-8b4ee157d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for proc in get_processes(process_ids):\n",
    "    try:\n",
    "        proc.kill()\n",
    "    except psutil.NoSuchProcess:\n",
    "        pass\n",
    "\n",
    "communities = []\n",
    "for filename in glob(\"./staging/*.pickle\"):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        communities += pickle.load(f)\n",
    "\n",
    "filename = \"communities.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(communities, f)\n",
    "communities = [x[1] for x in communities]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e48313-9e00-4e05-a222-3d494c7a8d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_currency_limits = {\n",
    "    \"btc\": 1,\n",
    "    \"eur\": 100,\n",
    "    \"usd\": 100,\n",
    "    \"gbp\": 100,\n",
    "    \"cad\": 100,\n",
    "    \"aud\": 100,\n",
    "    \"chf\": 100,\n",
    "    \"sar\": 1_000,\n",
    "    \"ils\": 1_000,\n",
    "    \"cny\": 1_000,\n",
    "    \"rub\": 5_000,\n",
    "    \"brl\": 5_000,\n",
    "    \"jpy\": 10_000,\n",
    "    \"mxn\": 10_000,\n",
    "    \"inr\": 10_000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e49032-f027-4637-8d51-ee490eeccb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_filtered_main = spark.read.parquet(s.STAGED_DATA_LOCATION)\n",
    "data_filtered_main = data_filtered_main.where(sf.col(\"format\").isin([\"ACH\", \"Bitcoin\"]))\n",
    "data_filtered = data_filtered_main.where(sf.lit(False))\n",
    "for currency, limit in temporal_currency_limits.items():\n",
    "    data_currency = data_filtered_main.where(\n",
    "        (sf.col(\"source_currency\") == currency) &\n",
    "        (sf.col(\"target_currency\") == currency)\n",
    "    ).where(sf.col(\"source_amount\") >= limit)\n",
    "    data_filtered = data_filtered.union(data_currency)\n",
    "edges_filtered = aggregate_edges(data_filtered)\n",
    "graph_filtered = ig.Graph.DataFrame(edges_filtered, use_vids=False, directed=True)\n",
    "nodes_filtered = [x[\"name\"] for x in graph_filtered.vs()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be91e4ae-2dd3-4687-9c73-16529665a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUMBER_OF_PROCESSES = 10\n",
    "\n",
    "shutil.rmtree(\"staging-filtered\", ignore_errors=True)\n",
    "os.mkdir(\"staging-filtered\")\n",
    "chunks = np.array_split(nodes_filtered, NUMBER_OF_PROCESSES)\n",
    "\n",
    "filename = \"graph_filtered.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(graph_filtered, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "filename = \"nodes_filtered.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(chunks, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "process_ids = set()\n",
    "process_name = \"communities_cycles.py\"\n",
    "for chunk_number in range(NUMBER_OF_PROCESSES):\n",
    "    process_id = str(uuid.uuid4())\n",
    "    process_ids = process_ids.union({process_id})\n",
    "    os.system(f\"{sys.executable} {process_name} {chunk_number} {process_id} &\")\n",
    "\n",
    "while get_processes(process_ids):\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89469745-f67c-4010-b08b-ef9a74aef25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for proc in get_processes(process_ids):\n",
    "    try:\n",
    "        proc.kill()\n",
    "    except psutil.NoSuchProcess:\n",
    "        pass\n",
    "\n",
    "communities_filtered = []\n",
    "for filename in glob(\"./staging-filtered/*.pickle\"):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        communities_filtered += pickle.load(f)\n",
    "\n",
    "filename = \"communities_filtered.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(communities_filtered, f)\n",
    "communities_filtered = [x[1] for x in communities_filtered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1638111-4639-4d70-ac0a-282e4fc8812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities += communities_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f09166-dad7-4283-8ae3-ce4af82ca34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [len(x) for x in communities]\n",
    "round(np.mean(sizes)), round(np.median(sizes)), round(np.max(sizes)), sum(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d0355-1158-45c2-8eb5-f10ea8a22e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = pd.read_parquet(\"flows.parquet\")\n",
    "flow_stats = pd.read_parquet(\"flow_stats.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38836d7f-aba3-442b-b491-358d233d9d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "search_hash = defaultdict(list)\n",
    "for index, community in enumerate(communities):\n",
    "    for node in community:\n",
    "        search_hash[node].append(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d535e4a4-5c66-47e4-be2b-2f58aed0e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "percentages = []\n",
    "start = time.time()\n",
    "for index, (group, grouped) in enumerate(flows.groupby(\"id\")):\n",
    "    flow_nodes = set(grouped[\"source\"]).union(grouped[\"target\"])\n",
    "    size = len(flow_nodes)\n",
    "    matches = []\n",
    "    perc = 0\n",
    "    for node in flow_nodes:\n",
    "        for i in search_hash[node]:\n",
    "            try:\n",
    "                matched_size = len(set(communities[i]).intersection(flow_nodes))\n",
    "            except KeyError:\n",
    "                continue\n",
    "            perc = matched_size / size\n",
    "            matches.append((node, perc))\n",
    "            if perc == 1:\n",
    "                break\n",
    "        if perc == 1:\n",
    "            break\n",
    "    matched_node_comm, perc = sorted(matches, reverse=True, key=lambda x: x[1])[0]\n",
    "    stats = flow_stats.loc[flow_stats[\"id\"] == group, :].iloc[0].to_dict()\n",
    "    stats[\"score\"] = perc\n",
    "    stats[\"matched_node_comm\"] = matched_node_comm\n",
    "    percentages.append(dict(stats))\n",
    "    if not (index % 2_000):\n",
    "        print(index, round(time.time() - start))\n",
    "        start = time.time()\n",
    "\n",
    "percentages = pd.DataFrame(percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21eeae-cb28-4933-b9ee-16f3381d38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered_main = data_filtered.select(\"*\")\n",
    "data_filtered_main.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc7795a-41d0-48a8-9db8-e3bd7d48611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(percentages[\"score\"].mean(), 2) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c37321-cb7a-4a1b-8777-a70d02ca5549",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(percentages[percentages[\"score\"] == 1].shape[0] / percentages.shape[0], 2) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6c149-b80c-4e1c-a832-ff24591b4172",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ = percentages[\"number_components\"] == 1\n",
    "percentages_scope = percentages.loc[filter_, :].reset_index(drop=True)\n",
    "percentages_scope.groupby(\"type\").agg({\"score\": \"mean\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0527e6-b545-4054-8549-a6b619966b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages_scope[percentages_scope[\"score\"] < 1].shape[0] / percentages_scope.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d13854-d051-40a6-b39d-1cb211658c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def left_column(column):\n",
    "#     return f\"{column}_left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f71b4bb-49ee-4ff1-9ea9-5c54ca2965b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOW_SIZE = 30\n",
    "# L1_AMOUNT = 1000\n",
    "# MAX_ALLOWED_DIFF_L1 = 0.15\n",
    "# MAX_ALLOWED_DIFF_L2 = 0.25\n",
    "\n",
    "# max_date = max(dates)\n",
    "# for date in dates:\n",
    "#     end_date = date + timedelta(days=WINDOW_SIZE)\n",
    "#     day = data_filtered.where(sf.col(\"timestamp\").astype(st.DateType()) == date)\n",
    "#     day = day.select(*[sf.col(x).alias(left_column(x)) for x in day.columns])\n",
    "#     window = data_filtered.where(\n",
    "#         (sf.col(\"timestamp\").astype(st.DateType()) >= date) &\n",
    "#         (sf.col(\"timestamp\").astype(st.DateType()) <= end_date)\n",
    "#     )\n",
    "#     joined = day.join(\n",
    "#         window, \n",
    "#         on=(\n",
    "#             (sf.col(\"timestamp\") >= sf.col(left_column(\"timestamp\"))) &\n",
    "#             (sf.col(\"source\") == sf.col(left_column(\"target\")))\n",
    "#         ),\n",
    "#         how=\"inner\"\n",
    "#     )\n",
    "#     join_diff_currency = joined.where(sf.col(\"currency\") != sf.col(left_column(\"currency\"))).withColumn(\n",
    "#         \"diff\", sf.lit(0)\n",
    "#     )\n",
    "#     join_same_currency = joined.where(sf.col(\"currency\") == sf.col(left_column(\"currency\"))).withColumn(\n",
    "#         \"diff\", \n",
    "#         sf.abs(sf.col(\"amount\") - sf.col(left_column(\"amount\"))) / (sf.col(\"amount\") - sf.col(left_column(\"amount\")))\n",
    "#     )\n",
    "#     join_same_currency_l1 = join_same_currency.where(\n",
    "#         (sf.col(\"amount\") <= L1_AMOUNT) | (sf.col(left_column(\"amount\")) <= L1_AMOUNT)\n",
    "#     )\n",
    "#     join_same_currency_l2 = join_same_currency.where(\n",
    "#         (sf.col(\"amount\") > L1_AMOUNT) & (sf.col(left_column(\"amount\")) > L1_AMOUNT)\n",
    "#     )\n",
    "#     combined = join_diff_currency.union(\n",
    "#         join_same_currency_l1.where(sf.col(\"diff\") <= MAX_ALLOWED_DIFF_L1)\n",
    "#     ).union(\n",
    "#         join_same_currency_l2.where(sf.col(\"diff\") <= MAX_ALLOWED_DIFF_L2)\n",
    "#     )\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d366e76-ffef-4a60-b49e-9757eb01af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = round(time.time() - start_script)\n",
    "print(f\"Script executed in {timedelta(seconds=delta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18b2d13-a460-4dc1-b3be-1524939a3834",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
