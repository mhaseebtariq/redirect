{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from itertools import combinations\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "\n",
    "import leidenalg as la\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "import settings as s\n",
    "from common import get_processes\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11827382-82ea-4d19-9880-569838c19725",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\", context=\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78718f0-7f15-4c1f-963f-bd160efec0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 11, 8):\n",
    "    raise EnvironmentError(\n",
    "        \"Only runs efficiently on Python 3.11.8 (Tested on: Conda 24.1.2 | Apple M3 Pro)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff6f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "    (\"spark.driver.memory\", \"16g\"),\n",
    "    (\"spark.worker.memory\", \"16g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"16g\"),\n",
    "]\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(config))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfc52ce-1f9d-49d2-81f7-366fcdab6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_script = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2442c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEGREE_PER_ACCOUNT = 100\n",
    "MAX_TRANSACTIONS_PER_ACCOUNT = 1_000\n",
    "\n",
    "data = spark.read.parquet(s.STAGED_DATA_LOCATION)\n",
    "\n",
    "#### [START] Seed selection ####\n",
    "data = data.where(sf.col(\"source\") != sf.col(\"target\"))\n",
    "data = data.where(sf.col(\"format\").isin([\"ACH\", \"Wire\", \"Bitcoin\"]))\n",
    "\n",
    "large_sources = (\n",
    "    data.groupby(\"source\")\n",
    "    .count()\n",
    "    .where(sf.col(\"count\") > MAX_TRANSACTIONS_PER_ACCOUNT)\n",
    "    .select(\"source\")\n",
    "    .toPandas()[\"source\"]\n",
    "    .tolist()\n",
    ")\n",
    "large_targets = (\n",
    "    data.groupby(\"target\")\n",
    "    .count()\n",
    "    .where(sf.col(\"count\") > MAX_TRANSACTIONS_PER_ACCOUNT)\n",
    "    .select(\"target\")\n",
    "    .toPandas()[\"target\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "large_sources = set(large_sources).union(\n",
    "    data.groupby(\"source\")\n",
    "    .agg(sf.countDistinct(\"target\").alias(\"count\"))\n",
    "    .where(sf.col(\"count\") > MAX_DEGREE_PER_ACCOUNT)\n",
    "    .select(\"source\")\n",
    "    .toPandas()[\"source\"]\n",
    "    .tolist()\n",
    ")\n",
    "large_targets = set(large_targets).union(\n",
    "    data.groupby(\"target\")\n",
    "    .agg(sf.countDistinct(\"source\").alias(\"count\"))\n",
    "    .where(sf.col(\"count\") > MAX_DEGREE_PER_ACCOUNT)\n",
    "    .select(\"target\")\n",
    "    .toPandas()[\"target\"]\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "data = data.where(~sf.col(\"source\").isin(large_sources))\n",
    "data = data.where(~sf.col(\"target\").isin(large_targets))\n",
    "#### [END] Seed selection ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54c5a1b-3375-4df8-b1d0-2dc7acc15356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_edges(data_input):\n",
    "    data_aggregated = (\n",
    "        data_input.groupby([\"source\", \"target\"])\n",
    "        .agg(\n",
    "            sf.sum(\"source_amount\").alias(\"source_amount\"),\n",
    "            sf.sum(\"target_amount\").alias(\"target_amount\"),\n",
    "        )\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    source_totals = (\n",
    "        data_aggregated.groupby(\"source\")\n",
    "        .agg({\"source_amount\": \"sum\"})[\"source_amount\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "    target_totals = (\n",
    "        data_aggregated.groupby(\"target\")\n",
    "        .agg({\"target_amount\": \"sum\"})[\"target_amount\"]\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    data_aggregated.loc[:, \"total_sent_by_source\"] = data_aggregated.loc[\n",
    "        :, \"source\"\n",
    "    ].apply(lambda x: source_totals[x])\n",
    "    data_aggregated.loc[:, \"total_received_by_target\"] = data_aggregated.loc[\n",
    "        :, \"target\"\n",
    "    ].apply(lambda x: target_totals[x])\n",
    "    data_aggregated.loc[:, \"weight\"] = data_aggregated.apply(\n",
    "        lambda x: (\n",
    "            (x[\"source_amount\"] / x[\"total_sent_by_source\"])\n",
    "            + (x[\"target_amount\"] / x[\"total_received_by_target\"])\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    data_aggregated.loc[:, \"source\"] = data_aggregated[\"source\"].str.slice(0, 8)\n",
    "    data_aggregated.loc[:, \"target\"] = data_aggregated[\"target\"].str.slice(0, 8)\n",
    "    filter_self = data_aggregated[\"source\"] != data_aggregated[\"target\"]\n",
    "    data_aggregated = data_aggregated.loc[filter_self, :].reset_index(drop=True)\n",
    "    return data_aggregated.loc[:, [\"source\", \"target\", \"weight\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c7dc18-939e-4345-9251-375c1cb9dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "edges = aggregate_edges(data)\n",
    "graph = ig.Graph.DataFrame(edges, use_vids=False, directed=True)\n",
    "nodes = [x[\"name\"] for x in graph.vs()]\n",
    "random.shuffle(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e714520-815c-426e-9551-dce041a430ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_mapping = {x.index: x[\"name\"] for x in graph.vs()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0293b2-a6f7-4d28-b1df-8b4ee157d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "communities = []\n",
    "for filename in glob(\"./staging/*.pickle\"):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        communities += pickle.load(f)\n",
    "\n",
    "original_size = len(communities)\n",
    "\n",
    "filename = \"communities.pickle\"\n",
    "with open(filename, \"wb\") as f:\n",
    "    pickle.dump(communities, f)\n",
    "\n",
    "communities_unique_rev = {}\n",
    "for comm_id, comm in communities:\n",
    "    communities_unique_rev[tuple(sorted(comm))] = comm_id\n",
    "\n",
    "communities_unique = {\n",
    "    f\"{v}-{i}\": k for i, (k, v) in enumerate(communities_unique_rev.items())\n",
    "}\n",
    "communities = list({x for x in communities_unique.values()})\n",
    "new_size = len(communities)\n",
    "\n",
    "print(original_size, new_size, round(new_size / original_size, 2))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f09166-dad7-4283-8ae3-ce4af82ca34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [len(x) for x in communities]\n",
    "round(np.mean(sizes)), round(np.median(sizes)), round(np.max(sizes)), sum(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c86162-7be8-4cc8-a2c6-726d3a645cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_rates = {\n",
    "    \"jpy\": np.float32(0.009487665410827868),\n",
    "    \"cny\": np.float32(0.14930721887033868),\n",
    "    \"cad\": np.float32(0.7579775434031815),\n",
    "    \"sar\": np.float32(0.2665884611958837),\n",
    "    \"aud\": np.float32(0.7078143121927827),\n",
    "    \"ils\": np.float32(0.29612081311363503),\n",
    "    \"chf\": np.float32(1.0928961554056371),\n",
    "    \"usd\": np.float32(1.0),\n",
    "    \"eur\": np.float32(1.171783425225877),\n",
    "    \"rub\": np.float32(0.012852809604990688),\n",
    "    \"gbp\": np.float32(1.2916554735187644),\n",
    "    \"btc\": np.float32(11879.132698717296),\n",
    "    \"inr\": np.float32(0.013615817231245796),\n",
    "    \"mxn\": np.float32(0.047296753463246695),\n",
    "    \"brl\": np.float32(0.1771008654705292),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359d0355-1158-45c2-8eb5-f10ea8a22e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = pd.read_parquet(\"flows.parquet\")\n",
    "flow_stats = pd.read_parquet(\"flow_stats.parquet\")\n",
    "flow_stats.loc[:, \"turnover_weight\"] = flow_stats.loc[:, \"turnover_weight\"].apply(\n",
    "    lambda x: json.loads(x)\n",
    ")\n",
    "flows.loc[:, \"amount_usd\"] = flows.apply(\n",
    "    lambda x: x[\"source_amount\"] * currency_rates[x[\"source_currency\"]], axis=1\n",
    ")\n",
    "ml_nodes = set(pd.read_parquet(\"ml_nodes.parquet\")[\"ml\"].tolist())\n",
    "single_node_flows = flow_stats[flow_stats[\"turnover_weight\"].apply(len) == 1][\n",
    "    \"id\"\n",
    "].tolist()\n",
    "flows = flows[~flows[\"id\"].isin(single_node_flows)].reset_index(drop=True)\n",
    "flow_stats = flow_stats[~flow_stats[\"id\"].isin(single_node_flows)].reset_index(\n",
    "    drop=True\n",
    ")\n",
    "print(flow_stats.shape[0], len(single_node_flows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd68d1c-ed9d-476f-ac60-17da1be9f2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_transactions = \"transactions_with_edges\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29292b2e-6d87-4c1b-859c-386074f2bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "columns = [\n",
    "    sf.substring(\"source\", 1, 8).alias(\"source\"),\n",
    "    sf.substring(\"target\", 1, 8).alias(\"target\"),\n",
    "    \"source_bank\",\n",
    "    \"target_bank\",\n",
    "    sf.unix_timestamp(\"timestamp\").alias(\"timestamp\"),\n",
    "    sf.col(\"source_amount\").alias(\"amount\"),\n",
    "    sf.col(\"source_currency\").alias(\"currency\"),\n",
    "]\n",
    "data.where(\n",
    "    (sf.col(\"source_currency\") == sf.col(\"target_currency\"))\n",
    "    & (sf.col(\"source\") != sf.col(\"target\"))\n",
    ").select(*columns).withColumn(\n",
    "    \"edge\",\n",
    "    sf.when(\n",
    "        sf.col(\"source\") < sf.col(\"target\"),\n",
    "        sf.concat(sf.col(\"source\"), sf.lit(\"-\"), sf.col(\"target\")),\n",
    "    ).otherwise(sf.concat(sf.col(\"target\"), sf.lit(\"-\"), sf.col(\"source\"))),\n",
    ").repartition(\n",
    "    1\n",
    ").write.parquet(\n",
    "    location_transactions, mode=\"overwrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a387620-8d6c-4d65-ba1d-90d4bf4752ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = pd.read_parquet(location_transactions).set_index(\"edge\")\n",
    "transactions.loc[:, \"amount_usd\"] = transactions.apply(\n",
    "    lambda x: currency_rates[x[\"currency\"]] * x[\"amount\"], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe501f5-ae37-4098-93c7-78719bb16322",
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"transactions_communities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f91b657-8703-42b4-94a1-07ed1e43be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "shutil.rmtree(location, ignore_errors=True)\n",
    "os.mkdir(location)\n",
    "\n",
    "shutil.rmtree(location, ignore_errors=True)\n",
    "os.mkdir(location)\n",
    "\n",
    "communities_keys = [x for x in communities_unique.keys()]\n",
    "\n",
    "number_of_chunks = int(np.ceil(len(communities_keys) / 50_000))\n",
    "chunks = np.array_split(communities_keys, number_of_chunks)\n",
    "for index, chunk in enumerate(chunks):\n",
    "    comm_inner = []\n",
    "    for key in chunk:\n",
    "        comm_node = communities_unique[key]\n",
    "        comm_inner += [[key, sorted(x)] for x in combinations(comm_node, 2)]\n",
    "    edge_combinations = pd.DataFrame(comm_inner, columns=[\"id\", \"edge\"])\n",
    "    edge_combinations.loc[:, \"edge\"] = edge_combinations.loc[:, \"edge\"].apply(\n",
    "        lambda x: f\"{x[0]}-{x[1]}\"\n",
    "    )\n",
    "    edge_combinations.set_index(\"edge\", inplace=True)\n",
    "    edge_combinations.join(transactions, how=\"inner\").reset_index(drop=True).to_parquet(\n",
    "        f\"{location}/part-{index}.parquet\"\n",
    "    )\n",
    "    if not (index % 20):\n",
    "        print(index, len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80615aa7-36ee-4d1a-9472-12795d48cd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_features_global = \"features_global\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884854a8-c8a5-4058-ae60-1cd6c5ea8c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "NUMBER_OF_PROCESSES = 10\n",
    "\n",
    "parts = sorted(\n",
    "    [x for x in glob(f\"{location}/*.parquet\")],\n",
    "    key=lambda x: int(x.split(\"-\")[-1].split(\".\")[0]),\n",
    ")\n",
    "\n",
    "shutil.rmtree(location_features_global, ignore_errors=True)\n",
    "os.mkdir(location_features_global)\n",
    "\n",
    "process_ids = set()\n",
    "process_name = \"features.py\"\n",
    "while parts:\n",
    "    if len(get_processes(process_ids)) < NUMBER_OF_PROCESSES:\n",
    "        process_id = str(uuid.uuid4())\n",
    "        process_ids = process_ids.union({process_id})\n",
    "        os.system(\n",
    "            f\"{sys.executable} {process_name} {parts.pop()} {location_features_global} {process_id} &\"\n",
    "        )\n",
    "\n",
    "while get_processes(process_ids):\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d0a3c3-c832-441c-a402-ce7212b262d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_N_ANOMALIES = 25_000\n",
    "# TOP_N_ANOMALIES = 50_000\n",
    "# TOP_N_ANOMALIES = 75_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b228ca-0299-49a9-999f-3c9326ed0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_global = pd.read_parquet(\"features_global\")\n",
    "features_global.loc[:, \"key_parent\"] = features_global.loc[:, \"key\"].apply(\n",
    "    lambda x: x.split(\"-\")[0]\n",
    ")\n",
    "\n",
    "predictions = features_global.loc[:, [\"key\", \"key_parent\", \"turnover\"]].copy(deep=True)\n",
    "del features_global[\"key\"]\n",
    "del features_global[\"key_parent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce02d2-c531-422c-8ce3-d048dc43cb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_turnover_score(df_input):\n",
    "    df_input.loc[:, \"turnover_score\"] = df_input[\"turnover\"] / 100_000\n",
    "    df_input.loc[df_input[\"turnover_score\"] > 1, \"turnover_score\"] = 1\n",
    "    return df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e09dfe-1d75-4fbd-9079-3aa8ccef457b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = IsolationForest()\n",
    "model.fit(features_global)\n",
    "predictions.loc[:, \"anomaly_score\"] = model.decision_function(features_global)\n",
    "predictions = add_turnover_score(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209dbfed-d475-46d7-95de-6a970972ce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_anomalies = predictions.loc[predictions.groupby(\"key_parent\")[\"key\"].idxmin(), :]\n",
    "top_anomalies = top_anomalies.sort_values(\"anomaly_score\").head(TOP_N_ANOMALIES)\n",
    "communities_shortlisted = {x: communities_unique[x] for x in top_anomalies[\"key\"]}\n",
    "print(f\"Number of commmununities: {len(communities_shortlisted):,}\")\n",
    "max_comm_size = max(\n",
    "    [len(x) for x in communities_shortlisted.values()] + [len(x) for x in flow_stats[\"turnover_weight\"]]\n",
    ") + 1\n",
    "print(f\"Max community size: {max_comm_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba168e8-118c-46dd-b09f-303dee03a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_stats.loc[:, \"turnover\"] = 0\n",
    "for key, df in flows.groupby(\"id\"):\n",
    "    left = (\n",
    "        df.loc[:, [\"target\", \"amount_usd\"]]\n",
    "        .rename(columns={\"target\": \"source\"})\n",
    "        .groupby(\"source\")\n",
    "        .agg({\"amount_usd\": \"sum\"})\n",
    "    )\n",
    "    right = df.groupby(\"source\").agg({\"amount_usd\": \"sum\"})\n",
    "    result = left.join(right, how=\"outer\", lsuffix=\"_left\").fillna(0).reset_index()\n",
    "    result.loc[:, \"delta\"] = result[\"amount_usd_left\"] - result[\"amount_usd\"]\n",
    "    turnover = float(result[result[\"delta\"] > 0][\"delta\"].sum())\n",
    "    flow_stats.loc[flow_stats[\"id\"] == key, \"turnover\"] = int(np.ceil(turnover))\n",
    "flow_stats.loc[:, \"turnover_score\"] = add_turnover_score(flow_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f331e6-ae09-4874-8f43-22e1599fa677",
   "metadata": {},
   "source": [
    "## Context-Weighted Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e1cfe-98d4-4e08-915e-aa29a0bf1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "matches = []\n",
    "not_found = []\n",
    "for index, stats in flow_stats.iterrows():\n",
    "    key = stats[\"id\"]\n",
    "    turnover_weight = stats[\"turnover_weight\"]\n",
    "    flow_nodes = set(turnover_weight.keys())\n",
    "    candidates = []\n",
    "    for comm_id, comm in communities_shortlisted.items():\n",
    "        matched = flow_nodes.intersection(comm)\n",
    "        if len(matched):\n",
    "            matched_score = sum([turnover_weight[x] for x in matched])\n",
    "            # We are not counting the isolated \"is_laundering\" transactions as true-positives,\n",
    "            # - though they should be not be counted towards false positive either\n",
    "            non_matched = set(comm) - ml_nodes\n",
    "            candidates.append((matched_score, matched, non_matched, turnover_weight, comm_id))\n",
    "    to_score = float(stats[\"turnover_score\"])\n",
    "    if candidates:\n",
    "        best = sorted(candidates, reverse=True, key=lambda x: (x[0], -len(x[2])))[0]\n",
    "        true_positives = best[0]\n",
    "        false_negatives = len(flow_nodes) - true_positives\n",
    "        false_positives = len(best[2])\n",
    "        true_negatives = max_comm_size - false_positives - len(flow_nodes)\n",
    "        total = int(round(sum((true_positives, false_negatives, false_positives, true_negatives))))\n",
    "        assert total == max_comm_size\n",
    "        true_positives /= max_comm_size\n",
    "        false_positives /= max_comm_size\n",
    "        true_negatives /= max_comm_size\n",
    "        false_negatives /= max_comm_size\n",
    "        matches.append(\n",
    "            (\n",
    "                true_positives * to_score, \n",
    "                false_positives * to_score, \n",
    "                true_negatives * to_score, \n",
    "                false_negatives * to_score, \n",
    "                best[4]\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        not_found.append((0, 0, 0, to_score, key))\n",
    "    if not (index % 5_000):\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbdec89-e61b-4c64-8ee7-d60a67944331",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matches = set(communities_shortlisted.keys()).difference([x[4] for x in matches])\n",
    "non_matches = [(x, len(communities_shortlisted[x])) for x in non_matches]\n",
    "non_matches_pd = predictions.loc[\n",
    "    predictions[\"key\"].isin([x[0] for x in non_matches]), :\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03766694-99fc-41d6-bfe7-7063d045cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_matches_cm = []\n",
    "for index, row in non_matches_pd.iterrows():\n",
    "    to_score = float(stats[\"turnover_score\"])\n",
    "    # We are not counting the isolated \"is_laundering\" transactions as true-positives,\n",
    "    # - though they should be not be counted towards false positive either\n",
    "    non_matched = set(comm) - ml_nodes\n",
    "    false_positives = len(non_matched)\n",
    "    true_negatives = max_comm_size - false_positives\n",
    "    false_positives /= max_comm_size\n",
    "    true_negatives /= max_comm_size\n",
    "    non_matches_cm.append((0, false_positives * to_score, true_negatives * to_score, 0, row[\"key\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654c1ac-de5d-4472-afb7-4702a4904ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_all, fp_all, tn_all, fn_all = 0, 0, 0, 0\n",
    "for tp, fp, tn, fn, _ in matches + non_matches_cm + not_found:\n",
    "    tp_all += tp\n",
    "    fp_all += fp\n",
    "    tn_all += tn\n",
    "    fn_all += fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3287ba-b51e-41d4-9dff-cd39ef6ac688",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(tp_all), int(fp_all), int(tn_all), int(fn_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795f88e9-885b-493d-9977-38018b133aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = (2 * tp_all) / ((2 * tp_all) + fp_all + fn_all)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51720c-435a-4d23-b8c0-037644118640",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp_all / (tp_all + fn_all)\n",
    "recall\n",
    "# 0.42 0.55 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e4ec9-993d-481a-8f9b-3cd31771849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp_all / (tp_all + fp_all)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63156ca6-6a65-4712-9ce6-c0c933780ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = round(time.time() - start_script)\n",
    "print(f\"Script executed in {timedelta(seconds=delta)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
