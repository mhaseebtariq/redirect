{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c949a975-6aac-497b-9680-8b708973d3db",
   "metadata": {},
   "source": [
    "### https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9762926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd66cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import settings as s\n",
    "from common import get_weights\n",
    "from communities import get_communities_spark\n",
    "from features import generate_features_spark, generate_features_udf_wrapper, SCHEMA_FEAT_UDF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78718f0-7f15-4c1f-963f-bd160efec0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 11, 8):\n",
    "    raise EnvironmentError(\n",
    "        \"Only runs efficiently on Python 3.11.8 (Tested on: Conda 24.1.2 | Apple M3 Pro)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f2aef-e6d2-48b2-b720-a02710e4e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_CONF = [\n",
    "    (\"spark.driver.memory\", \"32g\"),\n",
    "    (\"spark.worker.memory\", \"32g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"32g\"),\n",
    "    (\"spark.driver.bindAddress\", \"127.0.0.1\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "    (\"spark.network.timeout\", \"600s\"),\n",
    "    (\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "]\n",
    "\n",
    "shutil.rmtree(\"artifacts\", ignore_errors=True)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(SPARK_CONF))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2442c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/Libra_bank_3months_graph/data.csv\")\n",
    "rename = {\n",
    "    \"id_source\": \"source\",\n",
    "    \"id_destination\": \"target\",\n",
    "    \"cum_amount\": \"amount\",\n",
    "    \"nr_transactions\": \"num_transactions\",\n",
    "    \"nr_alerts\": \"alerts_count\",\n",
    "    \"nr_reports\": \"reports_count\",\n",
    "}\n",
    "\n",
    "data = data.rename(columns=rename)\n",
    "data.loc[:, \"source_\"] = (\n",
    "    data.loc[:, \"source\"].astype(str).apply(lambda x: f\"nid-{int(x)}\")\n",
    ")\n",
    "data.loc[:, \"target_\"] = (\n",
    "    data.loc[:, \"target\"].astype(str).apply(lambda x: f\"nid-{int(x)}\")\n",
    ")\n",
    "del data[\"source\"]\n",
    "del data[\"target\"]\n",
    "data = data.rename(\n",
    "    columns={\n",
    "        \"source_\": \"source\",\n",
    "        \"target_\": \"target\",\n",
    "    }\n",
    ").loc[:, [\"source\", \"target\", \"amount\", \"num_transactions\", \"alerts_count\", \"reports_count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab14662-5706-45b2-a53b-29b8b0b66760",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_data = pd.DataFrame(index=sorted(set(data[\"source\"].tolist() + data[\"target\"].tolist())))\n",
    "nodes_data.index.name = \"key\"\n",
    "\n",
    "w_alerts = int(data[\"alerts_count\"].sum() * 2)\n",
    "w_reports = int(data[\"reports_count\"].sum() * 2)\n",
    "\n",
    "w_alerts_source = data[data[\"alerts_count\"] > 0].groupby(\"source\").agg({\"alerts_count\": \"sum\"}).to_dict()[\"alerts_count\"]\n",
    "w_alerts_target = data[data[\"alerts_count\"] > 0].groupby(\"target\").agg({\"alerts_count\": \"sum\"}).to_dict()[\"alerts_count\"]\n",
    "\n",
    "w_reports_source = data[data[\"reports_count\"] > 0].groupby(\"source\").agg({\"reports_count\": \"sum\"}).to_dict()[\"reports_count\"]\n",
    "w_reports_target = data[data[\"reports_count\"] > 0].groupby(\"target\").agg({\"reports_count\": \"sum\"}).to_dict()[\"reports_count\"]\n",
    "\n",
    "nodes_data.loc[:, \"alert_weight\"] = nodes_data.index.map(\n",
    "    lambda x: (w_alerts_source.get(x, 0) + w_alerts_target.get(x, 0)) / w_alerts\n",
    ")\n",
    "nodes_data.loc[:, \"report_weight\"] = nodes_data.index.map(\n",
    "    lambda x: (w_reports_source.get(x, 0) + w_reports_target.get(x, 0)) / w_reports\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f09fa-58c6-4f8e-9804-1521ce72d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_nodes = nodes_data.shape[0]\n",
    "perc_point_1 = round(total_nodes * (0.1 / 100))\n",
    "perc_point_2 = round(total_nodes * (0.2 / 100))\n",
    "perc_point_5 = round(total_nodes * (0.5 / 100))\n",
    "perc_1_cnt = round(total_nodes * (1 / 100))\n",
    "perc_10_cnt = round(total_nodes * (10 / 100))\n",
    "perc_20_cnt = round(total_nodes * (20 / 100))\n",
    "perc_30_cnt = round(total_nodes * (30 / 100))\n",
    "perc_40_cnt = round(total_nodes * (40 / 100))\n",
    "perc_50_cnt = round(total_nodes * (50 / 100))\n",
    "perc_75_cnt = round(total_nodes * (75 / 100))\n",
    "print(total_nodes, perc_point_1, perc_point_2, perc_point_5, perc_1_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e2f8d1-77e5-4343-99ce-5f392b4b4b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "candidates = nodes_data.index.tolist()\n",
    "data_in_scope = data.copy(deep=True)\n",
    "data_in_scope = data_in_scope.set_index([\"source\", \"target\"]).join(\n",
    "    get_weights(data_in_scope).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "data_in_scope.loc[:, \"amount_weighted\"] = data_in_scope.loc[:, \"amount\"] * data_in_scope.loc[:, \"weight\"]\n",
    "\n",
    "%run model.ipynb\n",
    "\n",
    "anomalies_main = anomalies.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fbe31b-5c86-4bf0-aac0-f02144940b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predicted_alert_weight(anomalies_input, perc_count):\n",
    "    anomalies_perc_x = anomalies_input.copy(deep=True)\n",
    "    index = anomalies_perc_x.head(perc_count).index.tolist()\n",
    "    anomalies_perc_x = anomalies_perc_x.loc[index, :]\n",
    "    anomalies_perc_x.loc[:, \"predicted_alert_weight\"] = anomalies_perc_x.loc[:, \"alert_weight\"]\n",
    "    anomalies_perc_x.loc[:, \"predicted_report_weight\"] = anomalies_perc_x.loc[:, \"report_weight\"]\n",
    "    return anomalies_perc_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29c8983-063c-4fbe-8052-f1352de5772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_1_hop_dict = dict(communities_1_hop)\n",
    "sizes_1_hop_0_1_perc = [len(communities_1_hop_dict[x]) for x in add_predicted_alert_weight(anomalies_main, perc_point_1).index]\n",
    "sizes_1_hop_0_2_perc = [len(communities_1_hop_dict[x]) for x in add_predicted_alert_weight(anomalies_main, perc_point_2).index]\n",
    "sizes_1_hop_0_5_perc = [len(communities_1_hop_dict[x]) for x in add_predicted_alert_weight(anomalies_main, perc_point_5).index]\n",
    "sizes_1_hop_1_perc = [len(communities_1_hop_dict[x]) for x in add_predicted_alert_weight(anomalies_main, perc_1_cnt).index]\n",
    "\n",
    "results = [{\n",
    "    \"rectify_perc\": 1.0,\n",
    "    \"leiden_size_max\": np.max(sizes_leiden),\n",
    "    \"leiden_size_mean\": np.mean(sizes_leiden),\n",
    "    \"leiden_size_median\": np.median(sizes_leiden),\n",
    "    \"1_hop_size_max\": np.max(sizes_1_hop),\n",
    "    \"1_hop_size_mean\": np.mean(sizes_1_hop),\n",
    "    \"1_hop_size_median\": np.median(sizes_1_hop),\n",
    "    \"0.1%\": add_predicted_alert_weight(anomalies_main, perc_point_1)[\"predicted_alert_weight\"].sum(),\n",
    "    \"0.2%\": add_predicted_alert_weight(anomalies_main, perc_point_2)[\"predicted_alert_weight\"].sum(),\n",
    "    \"0.5%\": add_predicted_alert_weight(anomalies_main, perc_point_5)[\"predicted_alert_weight\"].sum(),\n",
    "    \"1%\": add_predicted_alert_weight(anomalies_main, perc_1_cnt)[\"predicted_alert_weight\"].sum(),\n",
    "    \"auc_1%\": np.mean(np.cumsum(add_predicted_alert_weight(anomalies_main, perc_1_cnt)[\"predicted_alert_weight\"])),\n",
    "    \"report_0.1%\": add_predicted_alert_weight(anomalies_main, perc_point_1)[\"predicted_report_weight\"].sum(),\n",
    "    \"report_0.2%\": add_predicted_alert_weight(anomalies_main, perc_point_2)[\"predicted_report_weight\"].sum(),\n",
    "    \"report_0.5%\": add_predicted_alert_weight(anomalies_main, perc_point_5)[\"predicted_report_weight\"].sum(),\n",
    "    \"report_1%\": add_predicted_alert_weight(anomalies_main, perc_1_cnt)[\"predicted_report_weight\"].sum(),\n",
    "    \"report_auc_1%\": np.mean(np.cumsum(add_predicted_alert_weight(anomalies_main, perc_1_cnt)[\"predicted_report_weight\"])),\n",
    "    \"max_1_hop_0_1_perc\": np.max(sizes_1_hop_0_1_perc),\n",
    "    \"max_1_hop_0_2_perc\": np.max(sizes_1_hop_0_2_perc),\n",
    "    \"max_1_hop_0_5_perc\": np.max(sizes_1_hop_0_5_perc),\n",
    "    \"max_1_hop_1_perc\": np.max(sizes_1_hop_1_perc),\n",
    "    \"mean_1_hop_0_1_perc\": np.mean(sizes_1_hop_0_1_perc),\n",
    "    \"mean_1_hop_0_2_perc\": np.mean(sizes_1_hop_0_2_perc),\n",
    "    \"mean_1_hop_0_5_perc\": np.mean(sizes_1_hop_0_5_perc),\n",
    "    \"mean_1_hop_1_perc\": np.mean(sizes_1_hop_1_perc),\n",
    "    \"median_1_hop_0_1_perc\": np.median(sizes_1_hop_0_1_perc),\n",
    "    \"median_1_hop_0_2_perc\": np.median(sizes_1_hop_0_2_perc),\n",
    "    \"median_1_hop_0_5_perc\": np.median(sizes_1_hop_0_5_perc),\n",
    "    \"median_1_hop_1_perc\": np.median(sizes_1_hop_1_perc),\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe80cc-d4c9-4c36-831f-a3c0a89f5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for perc in [perc_75_cnt, perc_50_cnt, perc_40_cnt, perc_30_cnt, perc_20_cnt, perc_10_cnt]:\n",
    "    rectify_perc = round(perc / total_nodes, 2)\n",
    "    candidates = add_predicted_alert_weight(anomalies_main, perc).index.tolist()\n",
    "    filter_ = data[\"source\"].isin(candidates) & data[\"target\"].isin(candidates)\n",
    "    data_in_scope = data.loc[filter_, :]\n",
    "    candidates = sorted(set(data_in_scope[\"source\"].unique()).union(data_in_scope[\"target\"].unique()))\n",
    "    print(\"=\" * 100)\n",
    "    print(rectify_perc, perc, len(candidates), data_in_scope.shape)\n",
    "    print(\"=\" * 100)\n",
    "    data_in_scope = data_in_scope.set_index([\"source\", \"target\"]).join(\n",
    "        get_weights(data_in_scope).set_index([\"source\", \"target\"]), how=\"left\"\n",
    "    ).reset_index()\n",
    "    data_in_scope.loc[:, \"amount_weighted\"] = data_in_scope.loc[:, \"amount\"] * data_in_scope.loc[:, \"weight\"]\n",
    "    \n",
    "    %run model.ipynb\n",
    "\n",
    "    communities_1_hop_dict = dict(communities_1_hop)\n",
    "    sizes_1_hop_0_1_perc = [len(communities_1_hop_dict[x]) for x in add_predicted_alert_weight(anomalies, perc_point_1).index]\n",
    "    sizes_1_hop_0_2_perc = [len(communities_1_hop_dict[x]) for x in add_predicted_alert_weight(anomalies, perc_point_2).index]\n",
    "    sizes_1_hop_0_5_perc = [len(communities_1_hop_dict[x]) for x in add_predicted_alert_weight(anomalies, perc_point_5).index]\n",
    "    sizes_1_hop_1_perc = [len(communities_1_hop_dict[x]) for x in add_predicted_alert_weight(anomalies, perc_1_cnt).index]\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        \"rectify_perc\": rectify_perc,\n",
    "        \"leiden_size_max\": np.max(sizes_leiden),\n",
    "        \"leiden_size_mean\": np.mean(sizes_leiden),\n",
    "        \"leiden_size_median\": np.median(sizes_leiden),\n",
    "        \"1_hop_size_max\": np.max(sizes_1_hop),\n",
    "        \"1_hop_size_mean\": np.mean(sizes_1_hop),\n",
    "        \"1_hop_size_median\": np.median(sizes_1_hop),\n",
    "        \"0.1%\": add_predicted_alert_weight(anomalies, perc_point_1)[\"predicted_alert_weight\"].sum(),\n",
    "        \"0.2%\": add_predicted_alert_weight(anomalies, perc_point_2)[\"predicted_alert_weight\"].sum(),\n",
    "        \"0.5%\": add_predicted_alert_weight(anomalies, perc_point_5)[\"predicted_alert_weight\"].sum(),\n",
    "        \"1%\": add_predicted_alert_weight(anomalies, perc_1_cnt)[\"predicted_alert_weight\"].sum(),\n",
    "        \"auc_1%\": np.mean(np.cumsum(add_predicted_alert_weight(anomalies, perc_1_cnt)[\"predicted_alert_weight\"])),\n",
    "        \"report_0.1%\": add_predicted_alert_weight(anomalies, perc_point_1)[\"predicted_report_weight\"].sum(),\n",
    "        \"report_0.2%\": add_predicted_alert_weight(anomalies, perc_point_2)[\"predicted_report_weight\"].sum(),\n",
    "        \"report_0.5%\": add_predicted_alert_weight(anomalies, perc_point_5)[\"predicted_report_weight\"].sum(),\n",
    "        \"report_1%\": add_predicted_alert_weight(anomalies, perc_1_cnt)[\"predicted_report_weight\"].sum(),\n",
    "        \"report_auc_1%\": np.mean(np.cumsum(add_predicted_alert_weight(anomalies, perc_1_cnt)[\"predicted_report_weight\"])),\n",
    "        \"max_1_hop_0_1_perc\": np.max(sizes_1_hop_0_1_perc),\n",
    "        \"max_1_hop_0_2_perc\": np.max(sizes_1_hop_0_2_perc),\n",
    "        \"max_1_hop_0_5_perc\": np.max(sizes_1_hop_0_5_perc),\n",
    "        \"max_1_hop_1_perc\": np.max(sizes_1_hop_1_perc),\n",
    "        \"mean_1_hop_0_1_perc\": np.mean(sizes_1_hop_0_1_perc),\n",
    "        \"mean_1_hop_0_2_perc\": np.mean(sizes_1_hop_0_2_perc),\n",
    "        \"mean_1_hop_0_5_perc\": np.mean(sizes_1_hop_0_5_perc),\n",
    "        \"mean_1_hop_1_perc\": np.mean(sizes_1_hop_1_perc),\n",
    "        \"median_1_hop_0_1_perc\": np.median(sizes_1_hop_0_1_perc),\n",
    "        \"median_1_hop_0_2_perc\": np.median(sizes_1_hop_0_2_perc),\n",
    "        \"median_1_hop_0_5_perc\": np.median(sizes_1_hop_0_5_perc),\n",
    "        \"median_1_hop_1_perc\": np.median(sizes_1_hop_1_perc),\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.to_parquet(\"results.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9e338-d4ff-4c68-b03f-d5c2175973c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
