{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c949a975-6aac-497b-9680-8b708973d3db",
   "metadata": {},
   "source": [
    "### https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9762926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd66cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import igraph as ig\n",
    "import leidenalg as la\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import settings as s\n",
    "from common import get_weights\n",
    "from communities import get_communities_spark\n",
    "from features import generate_features_spark, generate_features_udf_wrapper, SCHEMA_FEAT_UDF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78718f0-7f15-4c1f-963f-bd160efec0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 11, 8):\n",
    "    raise EnvironmentError(\n",
    "        \"Only runs efficiently on Python 3.11.8 (Tested on: Conda 24.1.2 | Apple M3 Pro)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285f2aef-e6d2-48b2-b720-a02710e4e26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/24 08:58:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "SPARK_CONF = [\n",
    "    (\"spark.driver.memory\", \"32g\"),\n",
    "    (\"spark.worker.memory\", \"32g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"32g\"),\n",
    "    (\"spark.driver.bindAddress\", \"127.0.0.1\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "    (\"spark.network.timeout\", \"600s\"),\n",
    "    (\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "]\n",
    "\n",
    "shutil.rmtree(\"artifacts\", ignore_errors=True)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(SPARK_CONF))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2442c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/Libra_bank_3months_graph/data.csv\")\n",
    "rename = {\n",
    "    \"id_source\": \"source\",\n",
    "    \"id_destination\": \"target\",\n",
    "    \"cum_amount\": \"amount\",\n",
    "    \"nr_transactions\": \"num_transactions\",\n",
    "    \"nr_alerts\": \"alerts_count\",\n",
    "    \"nr_reports\": \"reports_count\",\n",
    "}\n",
    "\n",
    "data = data.rename(columns=rename)\n",
    "data.loc[:, \"source_\"] = (\n",
    "    data.loc[:, \"source\"].astype(str).apply(lambda x: f\"nid-{int(x)}\")\n",
    ")\n",
    "data.loc[:, \"target_\"] = (\n",
    "    data.loc[:, \"target\"].astype(str).apply(lambda x: f\"nid-{int(x)}\")\n",
    ")\n",
    "del data[\"source\"]\n",
    "del data[\"target\"]\n",
    "data = data.rename(\n",
    "    columns={\n",
    "        \"source_\": \"source\",\n",
    "        \"target_\": \"target\",\n",
    "    }\n",
    ").loc[:, [\"source\", \"target\", \"amount\", \"num_transactions\", \"alerts_count\", \"reports_count\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dab14662-5706-45b2-a53b-29b8b0b66760",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_data = pd.DataFrame(index=sorted(set(data[\"source\"].tolist() + data[\"target\"].tolist())))\n",
    "nodes_data.index.name = \"key\"\n",
    "\n",
    "w_alerts = int(data[\"alerts_count\"].sum() * 2)\n",
    "w_reports = int(data[\"reports_count\"].sum() * 2)\n",
    "\n",
    "w_alerts_source = data[data[\"alerts_count\"] > 0].groupby(\"source\").agg({\"alerts_count\": \"sum\"}).to_dict()[\"alerts_count\"]\n",
    "w_alerts_target = data[data[\"alerts_count\"] > 0].groupby(\"target\").agg({\"alerts_count\": \"sum\"}).to_dict()[\"alerts_count\"]\n",
    "\n",
    "w_reports_source = data[data[\"reports_count\"] > 0].groupby(\"source\").agg({\"reports_count\": \"sum\"}).to_dict()[\"reports_count\"]\n",
    "w_reports_target = data[data[\"reports_count\"] > 0].groupby(\"target\").agg({\"reports_count\": \"sum\"}).to_dict()[\"reports_count\"]\n",
    "\n",
    "nodes_data.loc[:, \"alert_weight\"] = nodes_data.index.map(\n",
    "    lambda x: (w_alerts_source.get(x, 0) + w_alerts_target.get(x, 0)) / w_alerts\n",
    ")\n",
    "nodes_data.loc[:, \"report_weight\"] = nodes_data.index.map(\n",
    "    lambda x: (w_reports_source.get(x, 0) + w_reports_target.get(x, 0)) / w_reports\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8c6f09fa-58c6-4f8e-9804-1521ce72d70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385100 385 770 1926 3851\n"
     ]
    }
   ],
   "source": [
    "total_nodes = nodes_data.shape[0]\n",
    "perc_point_1_cnt = round(total_nodes * (0.1 / 100))\n",
    "perc_point_2_cnt = round(total_nodes * (0.2 / 100))\n",
    "perc_point_5_cnt = round(total_nodes * (0.5 / 100))\n",
    "perc_1_cnt = round(total_nodes * (1 / 100))\n",
    "perc_10_cnt = round(total_nodes * (10 / 100))\n",
    "perc_20_cnt = round(total_nodes * (20 / 100))\n",
    "perc_30_cnt = round(total_nodes * (30 / 100))\n",
    "perc_40_cnt = round(total_nodes * (40 / 100))\n",
    "perc_50_cnt = round(total_nodes * (50 / 100))\n",
    "perc_75_cnt = round(total_nodes * (75 / 100))\n",
    "print(total_nodes, perc_point_1_cnt, perc_point_2_cnt, perc_point_5_cnt, perc_1_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92e2f8d1-77e5-4343-99ce-5f392b4b4b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 08:58:20 WARN TaskSetManager: Stage 0 contains a task of very large size (3851 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing comm_as_source\n",
      "\n",
      "Processed hop #1 | 472,592 | 213,511\n",
      "Processed hop #2 | 6,730,058 | 194,847\n",
      "Processed hop #3 | 7,805,533 | 186,398\n",
      "Processed hop #4 | 8,894,931 | 185,538\n",
      "Processed hop #5 | 8,889,368 | 185,321\n",
      "\n",
      "Processing comm_as_target\n",
      "\n",
      "Processed hop #1 | 414,376 | 221,458\n",
      "Processed hop #2 | 6,111,806 | 201,104\n",
      "Processed hop #3 | 7,163,788 | 188,404\n",
      "Processed hop #4 | 8,469,276 | 186,909\n",
      "Processed hop #5 | 8,208,835 | 183,360\n",
      "\n",
      "Processing comm_as_passthrough\n",
      "\n",
      "Processed hop #1 | 251,417 | 49,869\n",
      "Processed hop #2 | 1,310,227 | 42,290\n",
      "Processed hop #3 | 1,725,981 | 40,671\n",
      "Processed hop #4 | 1,918,895 | 40,412\n",
      "Processed hop #5 | 1,952,010 | 40,363\n",
      "\n",
      "Processing comm_as_passthrough_reverse\n",
      "\n",
      "Processed hop #1 | 197,559 | 49,869\n",
      "Processed hop #2 | 1,081,845 | 41,267\n",
      "Processed hop #3 | 1,434,805 | 38,874\n",
      "Processed hop #4 | 1,719,403 | 38,462\n",
      "Processed hop #5 | 1,722,034 | 38,030\n",
      "\n",
      "\n",
      "comm_as_source_features\n",
      "\n",
      "CPU times: user 37.3 s, sys: 375 ms, total: 37.7 s\n",
      "Wall time: 37.7 s\n",
      "\n",
      "comm_as_target_features\n",
      "\n",
      "CPU times: user 37.5 s, sys: 186 ms, total: 37.7 s\n",
      "Wall time: 37.6 s\n",
      "\n",
      "comm_as_passthrough_features\n",
      "\n",
      "CPU times: user 8.26 s, sys: 48.7 ms, total: 8.31 s\n",
      "Wall time: 8.31 s\n",
      "\n",
      "comm_as_passthrough_features_reverse\n",
      "\n",
      "CPU times: user 7.8 s, sys: 41.2 ms, total: 7.84 s\n",
      "Wall time: 7.84 s\n",
      "\n",
      "\n",
      "CPU times: user 8min 7s, sys: 38.9 s, total: 8min 46s\n",
      "Wall time: 8min 51s\n",
      "Constructing Leiden communities\n",
      "CPU times: user 3min 49s, sys: 1.32 s, total: 3min 50s\n",
      "Wall time: 3min 50s\n",
      "Constructing 1-hop communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.16 s, sys: 83.6 ms, total: 2.24 s\n",
      "Wall time: 26.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.62 s, sys: 425 ms, total: 5.04 s\n",
      "Wall time: 41.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 9s, sys: 24.1 s, total: 2min 33s\n",
      "Wall time: 5min 34s\n",
      "1-hop-source features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 09:17:44 WARN TaskSetManager: Stage 12 contains a task of very large size (6867 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 s, sys: 242 ms, total: 1.93 s\n",
      "Wall time: 1min 13s\n",
      "1-hop-target features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 09:18:58 WARN TaskSetManager: Stage 15 contains a task of very large size (6961 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.71 s, sys: 227 ms, total: 1.94 s\n",
      "Wall time: 1min 16s\n",
      "Features: (385100, 176)\n",
      "Deleted 10 constant columns\n",
      "Script executed in 0:22:00\n",
      "Training the model\n",
      "CPU times: user 11min 7s, sys: 46.4 s, total: 11min 54s\n",
      "Wall time: 12min 26s\n",
      "CPU times: user 25min 33s, sys: 1min 52s, total: 27min 26s\n",
      "Wall time: 34min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "candidates = nodes_data.index.tolist()\n",
    "data_in_scope = data.copy(deep=True)\n",
    "data_in_scope = data_in_scope.set_index([\"source\", \"target\"]).join(\n",
    "    get_weights(data_in_scope).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "data_in_scope.loc[:, \"amount_weighted\"] = (\n",
    "    data_in_scope.loc[:, \"amount\"] * \n",
    "    (data_in_scope.loc[:, \"weight\"] / data_in_scope.loc[:, \"weight\"].max())\n",
    ")\n",
    "\n",
    "%run model.ipynb\n",
    "\n",
    "anomalies_main = anomalies.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd1182b-1edb-4d25-b16f-50c63c128e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_global = ig.Graph.DataFrame(data[[\"source\", \"target\"]], use_vids=False, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8fbe31b-5c86-4bf0-aac0-f02144940b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predicted_alert_weight(anomalies_input, perc_count_in):\n",
    "    anomalies_perc_x = anomalies_input.copy(deep=True)\n",
    "    index = anomalies_perc_x.head(perc_count_in).index.tolist()\n",
    "    anomalies_perc_x = anomalies_perc_x.loc[index, :]\n",
    "    anomalies_perc_x.loc[:, \"predicted_alert_weight\"] = anomalies_perc_x.loc[:, \"alert_weight\"]\n",
    "    anomalies_perc_x.loc[:, \"predicted_report_weight\"] = anomalies_perc_x.loc[:, \"report_weight\"]\n",
    "    return anomalies_perc_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d29c8983-063c-4fbe-8052-f1352de5772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "communities_1_hop_dict = dict(communities_1_hop)\n",
    "\n",
    "comms_1_hop_0_1_perc = [communities_1_hop_dict[x] for x in add_predicted_alert_weight(anomalies_main, perc_point_1_cnt).index]\n",
    "comms_1_hop_0_2_perc = [communities_1_hop_dict[x] for x in add_predicted_alert_weight(anomalies_main, perc_point_2_cnt).index]\n",
    "comms_1_hop_0_5_perc = [communities_1_hop_dict[x] for x in add_predicted_alert_weight(anomalies_main, perc_point_5_cnt).index]\n",
    "comms_1_hop_1_perc = [communities_1_hop_dict[x] for x in add_predicted_alert_weight(anomalies_main, perc_1_cnt).index]\n",
    "\n",
    "sizes_1_hop_0_1_perc = [len(x) for x in comms_1_hop_0_1_perc]\n",
    "sizes_1_hop_0_2_perc = [len(x) for x in comms_1_hop_0_2_perc]\n",
    "sizes_1_hop_0_5_perc = [len(x) for x in comms_1_hop_0_5_perc]\n",
    "sizes_1_hop_1_perc = [len(x) for x in comms_1_hop_1_perc]\n",
    "\n",
    "sizes_edge_1_hop_0_1_perc = [graph_global.induced_subgraph(x).ecount() for x in comms_1_hop_0_1_perc]\n",
    "sizes_edge_1_hop_0_2_perc = [graph_global.induced_subgraph(x).ecount() for x in comms_1_hop_0_2_perc]\n",
    "sizes_edge_1_hop_0_5_perc = [graph_global.induced_subgraph(x).ecount() for x in comms_1_hop_0_5_perc]\n",
    "sizes_edge_1_hop_1_perc = [graph_global.induced_subgraph(x).ecount() for x in comms_1_hop_1_perc]\n",
    "\n",
    "results = [{\n",
    "    \"rectify_perc\": 1.0,\n",
    "    \"leiden_size_max\": np.max(sizes_leiden),\n",
    "    \"leiden_size_mean\": np.mean(sizes_leiden),\n",
    "    \"leiden_size_median\": np.median(sizes_leiden),\n",
    "    \"1_hop_size_max\": np.max(sizes_1_hop),\n",
    "    \"1_hop_size_mean\": np.mean(sizes_1_hop),\n",
    "    \"1_hop_size_median\": np.median(sizes_1_hop),\n",
    "    \"0.1%\": add_predicted_alert_weight(anomalies_main, perc_point_1_cnt)[\"predicted_alert_weight\"].sum(),\n",
    "    \"0.2%\": add_predicted_alert_weight(anomalies_main, perc_point_2_cnt)[\"predicted_alert_weight\"].sum(),\n",
    "    \"0.5%\": add_predicted_alert_weight(anomalies_main, perc_point_5_cnt)[\"predicted_alert_weight\"].sum(),\n",
    "    \"1%\": add_predicted_alert_weight(anomalies_main, perc_1_cnt)[\"predicted_alert_weight\"].sum(),\n",
    "    \"auc_1%\": np.mean(np.cumsum(add_predicted_alert_weight(anomalies_main, perc_1_cnt)[\"predicted_alert_weight\"])),\n",
    "    \"report_0.1%\": add_predicted_alert_weight(anomalies_main, perc_point_1_cnt)[\"predicted_report_weight\"].sum(),\n",
    "    \"report_0.2%\": add_predicted_alert_weight(anomalies_main, perc_point_2_cnt)[\"predicted_report_weight\"].sum(),\n",
    "    \"report_0.5%\": add_predicted_alert_weight(anomalies_main, perc_point_5_cnt)[\"predicted_report_weight\"].sum(),\n",
    "    \"report_1%\": add_predicted_alert_weight(anomalies_main, perc_1_cnt)[\"predicted_report_weight\"].sum(),\n",
    "    \"report_auc_1%\": np.mean(np.cumsum(add_predicted_alert_weight(anomalies_main, perc_1_cnt)[\"predicted_report_weight\"])),\n",
    "    \"max_1_hop_0_1_perc\": np.max(sizes_1_hop_0_1_perc),\n",
    "    \"max_1_hop_0_2_perc\": np.max(sizes_1_hop_0_2_perc),\n",
    "    \"max_1_hop_0_5_perc\": np.max(sizes_1_hop_0_5_perc),\n",
    "    \"max_1_hop_1_perc\": np.max(sizes_1_hop_1_perc),\n",
    "    \"mean_1_hop_0_1_perc\": np.mean(sizes_1_hop_0_1_perc),\n",
    "    \"mean_1_hop_0_2_perc\": np.mean(sizes_1_hop_0_2_perc),\n",
    "    \"mean_1_hop_0_5_perc\": np.mean(sizes_1_hop_0_5_perc),\n",
    "    \"mean_1_hop_1_perc\": np.mean(sizes_1_hop_1_perc),\n",
    "    \"median_1_hop_0_1_perc\": np.median(sizes_1_hop_0_1_perc),\n",
    "    \"median_1_hop_0_2_perc\": np.median(sizes_1_hop_0_2_perc),\n",
    "    \"median_1_hop_0_5_perc\": np.median(sizes_1_hop_0_5_perc),\n",
    "    \"median_1_hop_1_perc\": np.median(sizes_1_hop_1_perc),\n",
    "    \"max_edges_1_hop_0_1_perc\": np.max(sizes_edge_1_hop_0_1_perc),\n",
    "    \"max_edges_1_hop_0_2_perc\": np.max(sizes_edge_1_hop_0_2_perc),\n",
    "    \"max_edges_1_hop_0_5_perc\": np.max(sizes_edge_1_hop_0_5_perc),\n",
    "    \"max_edges_1_hop_1_perc\": np.max(sizes_edge_1_hop_1_perc),\n",
    "    \"mean_edges_1_hop_0_1_perc\": np.mean(sizes_edge_1_hop_0_1_perc),\n",
    "    \"mean_edges_1_hop_0_2_perc\": np.mean(sizes_edge_1_hop_0_2_perc),\n",
    "    \"mean_edges_1_hop_0_5_perc\": np.mean(sizes_edge_1_hop_0_5_perc),\n",
    "    \"mean_edges_1_hop_1_perc\": np.mean(sizes_edge_1_hop_1_perc),\n",
    "    \"median_edges_1_hop_0_1_perc\": np.median(sizes_edge_1_hop_0_1_perc),\n",
    "    \"median_edges_1_hop_0_2_perc\": np.median(sizes_edge_1_hop_0_2_perc),\n",
    "    \"median_edges_1_hop_0_5_perc\": np.median(sizes_edge_1_hop_0_5_perc),\n",
    "    \"median_edges_1_hop_1_perc\": np.median(sizes_edge_1_hop_1_perc),\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "acfe80cc-d4c9-4c36-831f-a3c0a89f5e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "0.5 192550 190877 385100 (399355, 6)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 10:09:55 WARN TaskSetManager: Stage 45 contains a task of very large size (2599 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing comm_as_source\n",
      "\n",
      "Processed hop #1 | 312,064 | 107,474\n",
      "Processed hop #2 | 3,653,958 | 99,646\n",
      "Processed hop #3 | 4,314,776 | 97,782\n",
      "Processed hop #4 | 4,708,516 | 97,513\n",
      "Processed hop #5 | 4,770,342 | 97,465\n",
      "\n",
      "Processing comm_as_target\n",
      "\n",
      "Processed hop #1 | 273,195 | 128,438\n",
      "Processed hop #2 | 4,193,500 | 119,453\n",
      "Processed hop #3 | 4,841,506 | 116,285\n",
      "Processed hop #4 | 5,415,980 | 115,780\n",
      "Processed hop #5 | 5,401,775 | 115,046\n",
      "\n",
      "Processing comm_as_passthrough\n",
      "\n",
      "Processed hop #1 | 199,876 | 45,035\n",
      "Processed hop #2 | 1,217,622 | 40,209\n",
      "Processed hop #3 | 1,647,027 | 39,269\n",
      "Processed hop #4 | 1,854,648 | 39,097\n",
      "Processed hop #5 | 1,897,203 | 39,067\n",
      "\n",
      "Processing comm_as_passthrough_reverse\n",
      "\n",
      "Processed hop #1 | 150,970 | 45,035\n",
      "Processed hop #2 | 969,388 | 39,053\n",
      "Processed hop #3 | 1,362,601 | 37,423\n",
      "Processed hop #4 | 1,648,516 | 37,124\n",
      "Processed hop #5 | 1,690,671 | 36,914\n",
      "\n",
      "\n",
      "comm_as_source_features\n",
      "\n",
      "CPU times: user 19.6 s, sys: 192 ms, total: 19.8 s\n",
      "Wall time: 19.8 s\n",
      "\n",
      "comm_as_target_features\n",
      "\n",
      "CPU times: user 23.9 s, sys: 237 ms, total: 24.2 s\n",
      "Wall time: 24.2 s\n",
      "\n",
      "comm_as_passthrough_features\n",
      "\n",
      "CPU times: user 9.25 s, sys: 199 ms, total: 9.45 s\n",
      "Wall time: 9.47 s\n",
      "\n",
      "comm_as_passthrough_features_reverse\n",
      "\n",
      "CPU times: user 7.95 s, sys: 86.9 ms, total: 8.04 s\n",
      "Wall time: 8.01 s\n",
      "\n",
      "\n",
      "CPU times: user 5min 13s, sys: 23.2 s, total: 5min 36s\n",
      "Wall time: 5min 38s\n",
      "Constructing Leiden communities\n",
      "CPU times: user 2min 12s, sys: 534 ms, total: 2min 12s\n",
      "Wall time: 2min 12s\n",
      "Constructing 1-hop communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.43 s, sys: 28.2 ms, total: 1.46 s\n",
      "Wall time: 6.05 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.28 s, sys: 47.9 ms, total: 2.33 s\n",
      "Wall time: 28.5 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min, sys: 485 ms, total: 1min 1s\n",
      "Wall time: 2min 32s\n",
      "1-hop-source features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 10:20:53 WARN TaskSetManager: Stage 57 contains a task of very large size (4637 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 934 ms, sys: 139 ms, total: 1.07 s\n",
      "Wall time: 38.3 s\n",
      "1-hop-target features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 10:21:31 WARN TaskSetManager: Stage 60 contains a task of very large size (4679 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.11 s, sys: 152 ms, total: 1.26 s\n",
      "Wall time: 45.9 s\n",
      "Features: (190877, 176)\n",
      "Deleted 10 constant columns\n",
      "Script executed in 0:12:25\n",
      "Training the model\n",
      "CPU times: user 6min 27s, sys: 9.14 s, total: 6min 36s\n",
      "Wall time: 6min 36s\n",
      "====================================================================================================\n",
      "0.25 95438 95286 385100 (290730, 6)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 10:28:58 WARN TaskSetManager: Stage 63 contains a task of very large size (1905 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing comm_as_source\n",
      "\n",
      "Processed hop #1 | 251,781 | 66,913\n",
      "Processed hop #2 | 1,981,864 | 61,601\n",
      "Processed hop #3 | 2,571,844 | 60,391\n",
      "Processed hop #4 | 2,872,748 | 60,212\n",
      "Processed hop #5 | 2,934,594 | 60,175\n",
      "\n",
      "Processing comm_as_target\n",
      "\n",
      "Processed hop #1 | 200,623 | 70,206\n",
      "Processed hop #2 | 1,743,199 | 63,710\n",
      "Processed hop #3 | 2,290,611 | 61,666\n",
      "Processed hop #4 | 2,759,919 | 61,302\n",
      "Processed hop #5 | 2,838,178 | 61,127\n",
      "\n",
      "Processing comm_as_passthrough\n",
      "\n",
      "Processed hop #1 | 184,804 | 41,833\n",
      "Processed hop #2 | 1,148,458 | 38,162\n",
      "Processed hop #3 | 1,577,373 | 37,475\n",
      "Processed hop #4 | 1,773,189 | 37,351\n",
      "Processed hop #5 | 1,814,461 | 37,330\n",
      "\n",
      "Processing comm_as_passthrough_reverse\n",
      "\n",
      "Processed hop #1 | 139,512 | 41,833\n",
      "Processed hop #2 | 907,318 | 37,004\n",
      "Processed hop #3 | 1,299,913 | 35,792\n",
      "Processed hop #4 | 1,579,957 | 35,561\n",
      "Processed hop #5 | 1,634,149 | 35,419\n",
      "\n",
      "\n",
      "comm_as_source_features\n",
      "\n",
      "CPU times: user 12.1 s, sys: 128 ms, total: 12.2 s\n",
      "Wall time: 12.2 s\n",
      "\n",
      "comm_as_target_features\n",
      "\n",
      "CPU times: user 12.2 s, sys: 41.1 ms, total: 12.2 s\n",
      "Wall time: 12.2 s\n",
      "\n",
      "comm_as_passthrough_features\n",
      "\n",
      "CPU times: user 7.5 s, sys: 20.2 ms, total: 7.52 s\n",
      "Wall time: 7.52 s\n",
      "\n",
      "comm_as_passthrough_features_reverse\n",
      "\n",
      "CPU times: user 7.2 s, sys: 31.7 ms, total: 7.23 s\n",
      "Wall time: 7.23 s\n",
      "\n",
      "\n",
      "CPU times: user 3min 23s, sys: 12.5 s, total: 3min 36s\n",
      "Wall time: 3min 37s\n",
      "Constructing Leiden communities\n",
      "CPU times: user 1min 17s, sys: 402 ms, total: 1min 17s\n",
      "Wall time: 1min 17s\n",
      "Constructing 1-hop communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 768 ms, sys: 15.5 ms, total: 784 ms\n",
      "Wall time: 2.42 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.19 s, sys: 38.2 ms, total: 1.23 s\n",
      "Wall time: 6.05 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.6 s, sys: 222 ms, total: 28.9 s\n",
      "Wall time: 1min 25s\n",
      "1-hop-source features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 10:35:28 WARN TaskSetManager: Stage 75 contains a task of very large size (3398 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 577 ms, sys: 84.8 ms, total: 662 ms\n",
      "Wall time: 24.1 s\n",
      "1-hop-target features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 10:35:52 WARN TaskSetManager: Stage 78 contains a task of very large size (3419 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 600 ms, sys: 92.1 ms, total: 692 ms\n",
      "Wall time: 24.4 s\n",
      "Features: (95286, 176)\n",
      "Deleted 10 constant columns\n",
      "Script executed in 0:07:19\n",
      "Training the model\n",
      "CPU times: user 3min 49s, sys: 2.42 s, total: 3min 51s\n",
      "Wall time: 3min 51s\n",
      "====================================================================================================\n",
      "0.12 47643 47412 385100 (185085, 6)\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 10:40:10 WARN TaskSetManager: Stage 81 contains a task of very large size (1221 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing comm_as_source\n",
      "\n",
      "Processed hop #1 | 158,083 | 33,076\n",
      "Processed hop #2 | 1,005,986 | 31,430\n",
      "Processed hop #3 | 1,352,794 | 30,984\n",
      "Processed hop #4 | 1,483,752 | 30,928\n",
      "Processed hop #5 | 1,513,903 | 30,914\n",
      "\n",
      "Processing comm_as_target\n",
      "\n",
      "Processed hop #1 | 126,670 | 38,202\n",
      "Processed hop #2 | 1,105,919 | 35,918\n",
      "Processed hop #3 | 1,410,338 | 35,201\n",
      "Processed hop #4 | 1,614,997 | 35,078\n",
      "Processed hop #5 | 1,668,112 | 35,038\n",
      "\n",
      "Processing comm_as_passthrough\n",
      "\n",
      "Processed hop #1 | 125,941 | 23,866\n",
      "Processed hop #2 | 704,898 | 22,790\n",
      "Processed hop #3 | 978,626 | 22,530\n",
      "Processed hop #4 | 1,075,292 | 22,496\n",
      "Processed hop #5 | 1,098,903 | 22,489\n",
      "\n",
      "Processing comm_as_passthrough_reverse\n",
      "\n",
      "Processed hop #1 | 94,466 | 23,866\n",
      "Processed hop #2 | 553,503 | 22,126\n",
      "Processed hop #3 | 807,473 | 21,659\n",
      "Processed hop #4 | 966,887 | 21,574\n",
      "Processed hop #5 | 1,012,571 | 21,557\n",
      "\n",
      "\n",
      "comm_as_source_features\n",
      "\n",
      "CPU times: user 6.1 s, sys: 25.4 ms, total: 6.13 s\n",
      "Wall time: 6.13 s\n",
      "\n",
      "comm_as_target_features\n",
      "\n",
      "CPU times: user 6.89 s, sys: 29.6 ms, total: 6.92 s\n",
      "Wall time: 6.91 s\n",
      "\n",
      "comm_as_passthrough_features\n",
      "\n",
      "CPU times: user 4.43 s, sys: 14.9 ms, total: 4.45 s\n",
      "Wall time: 4.45 s\n",
      "\n",
      "comm_as_passthrough_features_reverse\n",
      "\n",
      "CPU times: user 4.18 s, sys: 15.5 ms, total: 4.2 s\n",
      "Wall time: 4.19 s\n",
      "\n",
      "\n",
      "CPU times: user 1min 53s, sys: 6.89 s, total: 2min\n",
      "Wall time: 2min 1s\n",
      "Constructing Leiden communities\n",
      "CPU times: user 43.1 s, sys: 216 ms, total: 43.3 s\n",
      "Wall time: 43.3 s\n",
      "Constructing 1-hop communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 210 ms, sys: 10.4 ms, total: 220 ms\n",
      "Wall time: 1.59 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 583 ms, sys: 26 ms, total: 609 ms\n",
      "Wall time: 3.31 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.9 s, sys: 132 ms, total: 15.1 s\n",
      "Wall time: 43 s\n",
      "1-hop-source features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 10:43:43 WARN TaskSetManager: Stage 93 contains a task of very large size (2177 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 290 ms, sys: 49.6 ms, total: 339 ms\n",
      "Wall time: 12.7 s\n",
      "1-hop-target features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/24 10:43:55 WARN TaskSetManager: Stage 96 contains a task of very large size (2185 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 339 ms, sys: 55.8 ms, total: 395 ms\n",
      "Wall time: 13.7 s\n",
      "Features: (47412, 176)\n",
      "Deleted 10 constant columns\n",
      "Script executed in 0:04:00\n",
      "Training the model\n",
      "CPU times: user 2min 27s, sys: 1.26 s, total: 2min 29s\n",
      "Wall time: 2min 29s\n",
      "CPU times: user 29min 32s, sys: 59.1 s, total: 30min 31s\n",
      "Wall time: 36min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "anomalies = anomalies_main.copy(deep=True)\n",
    "for perc in [0.5, 0.5, 0.5]:\n",
    "    total_nodes = len(anomalies)\n",
    "    perc_count = round(perc * total_nodes)\n",
    "    rectify_perc = round(perc_count / len(anomalies_main), 2)\n",
    "    candidates = add_predicted_alert_weight(anomalies, perc_count).index.tolist()\n",
    "    filter_ = data[\"source\"].isin(candidates) & data[\"target\"].isin(candidates)\n",
    "    data_in_scope = data.loc[filter_, :]\n",
    "    candidates = sorted(set(data_in_scope[\"source\"].unique()).union(data_in_scope[\"target\"].unique()))\n",
    "    print(\"=\" * 100)\n",
    "    print(rectify_perc, perc_count, len(candidates), len(anomalies_main), data_in_scope.shape)\n",
    "    print(\"=\" * 100)\n",
    "    data_in_scope = data_in_scope.set_index([\"source\", \"target\"]).join(\n",
    "        get_weights(data_in_scope).set_index([\"source\", \"target\"]), how=\"left\"\n",
    "    ).reset_index()\n",
    "    data_in_scope.loc[:, \"amount_weighted\"] = data_in_scope.loc[:, \"amount\"] * data_in_scope.loc[:, \"weight\"]\n",
    "    \n",
    "    %run model.ipynb\n",
    "\n",
    "    communities_1_hop_dict = dict(communities_1_hop)\n",
    "\n",
    "    comms_1_hop_0_1_perc = [communities_1_hop_dict[x] for x in add_predicted_alert_weight(anomalies, perc_point_1_cnt).index]\n",
    "    comms_1_hop_0_2_perc = [communities_1_hop_dict[x] for x in add_predicted_alert_weight(anomalies, perc_point_2_cnt).index]\n",
    "    comms_1_hop_0_5_perc = [communities_1_hop_dict[x] for x in add_predicted_alert_weight(anomalies, perc_point_5_cnt).index]\n",
    "    comms_1_hop_1_perc = [communities_1_hop_dict[x] for x in add_predicted_alert_weight(anomalies, perc_1_cnt).index]\n",
    "    \n",
    "    sizes_1_hop_0_1_perc = [len(x) for x in comms_1_hop_0_1_perc]\n",
    "    sizes_1_hop_0_2_perc = [len(x) for x in comms_1_hop_0_2_perc]\n",
    "    sizes_1_hop_0_5_perc = [len(x) for x in comms_1_hop_0_5_perc]\n",
    "    sizes_1_hop_1_perc = [len(x) for x in comms_1_hop_1_perc]\n",
    "    \n",
    "    sizes_edge_1_hop_0_1_perc = [graph_global.induced_subgraph(x).ecount() for x in comms_1_hop_0_1_perc]\n",
    "    sizes_edge_1_hop_0_2_perc = [graph_global.induced_subgraph(x).ecount() for x in comms_1_hop_0_2_perc]\n",
    "    sizes_edge_1_hop_0_5_perc = [graph_global.induced_subgraph(x).ecount() for x in comms_1_hop_0_5_perc]\n",
    "    sizes_edge_1_hop_1_perc = [graph_global.induced_subgraph(x).ecount() for x in comms_1_hop_1_perc]\n",
    "\n",
    "    results.append({\n",
    "        \"rectify_perc\": rectify_perc,\n",
    "        \"leiden_size_max\": np.max(sizes_leiden),\n",
    "        \"leiden_size_mean\": np.mean(sizes_leiden),\n",
    "        \"leiden_size_median\": np.median(sizes_leiden),\n",
    "        \"1_hop_size_max\": np.max(sizes_1_hop),\n",
    "        \"1_hop_size_mean\": np.mean(sizes_1_hop),\n",
    "        \"1_hop_size_median\": np.median(sizes_1_hop),\n",
    "        \"0.1%\": add_predicted_alert_weight(anomalies, perc_point_1_cnt)[\"predicted_alert_weight\"].sum(),\n",
    "        \"0.2%\": add_predicted_alert_weight(anomalies, perc_point_2_cnt)[\"predicted_alert_weight\"].sum(),\n",
    "        \"0.5%\": add_predicted_alert_weight(anomalies, perc_point_5_cnt)[\"predicted_alert_weight\"].sum(),\n",
    "        \"1%\": add_predicted_alert_weight(anomalies, perc_1_cnt)[\"predicted_alert_weight\"].sum(),\n",
    "        \"auc_1%\": np.mean(np.cumsum(add_predicted_alert_weight(anomalies, perc_1_cnt)[\"predicted_alert_weight\"])),\n",
    "        \"report_0.1%\": add_predicted_alert_weight(anomalies, perc_point_1_cnt)[\"predicted_report_weight\"].sum(),\n",
    "        \"report_0.2%\": add_predicted_alert_weight(anomalies, perc_point_2_cnt)[\"predicted_report_weight\"].sum(),\n",
    "        \"report_0.5%\": add_predicted_alert_weight(anomalies, perc_point_5_cnt)[\"predicted_report_weight\"].sum(),\n",
    "        \"report_1%\": add_predicted_alert_weight(anomalies, perc_1_cnt)[\"predicted_report_weight\"].sum(),\n",
    "        \"report_auc_1%\": np.mean(np.cumsum(add_predicted_alert_weight(anomalies, perc_1_cnt)[\"predicted_report_weight\"])),\n",
    "        \"max_1_hop_0_1_perc\": np.max(sizes_1_hop_0_1_perc),\n",
    "        \"max_1_hop_0_2_perc\": np.max(sizes_1_hop_0_2_perc),\n",
    "        \"max_1_hop_0_5_perc\": np.max(sizes_1_hop_0_5_perc),\n",
    "        \"max_1_hop_1_perc\": np.max(sizes_1_hop_1_perc),\n",
    "        \"mean_1_hop_0_1_perc\": np.mean(sizes_1_hop_0_1_perc),\n",
    "        \"mean_1_hop_0_2_perc\": np.mean(sizes_1_hop_0_2_perc),\n",
    "        \"mean_1_hop_0_5_perc\": np.mean(sizes_1_hop_0_5_perc),\n",
    "        \"mean_1_hop_1_perc\": np.mean(sizes_1_hop_1_perc),\n",
    "        \"median_1_hop_0_1_perc\": np.median(sizes_1_hop_0_1_perc),\n",
    "        \"median_1_hop_0_2_perc\": np.median(sizes_1_hop_0_2_perc),\n",
    "        \"median_1_hop_0_5_perc\": np.median(sizes_1_hop_0_5_perc),\n",
    "        \"median_1_hop_1_perc\": np.median(sizes_1_hop_1_perc),\n",
    "        \"max_edges_1_hop_0_1_perc\": np.max(sizes_edge_1_hop_0_1_perc),\n",
    "        \"max_edges_1_hop_0_2_perc\": np.max(sizes_edge_1_hop_0_2_perc),\n",
    "        \"max_edges_1_hop_0_5_perc\": np.max(sizes_edge_1_hop_0_5_perc),\n",
    "        \"max_edges_1_hop_1_perc\": np.max(sizes_edge_1_hop_1_perc),\n",
    "        \"mean_edges_1_hop_0_1_perc\": np.mean(sizes_edge_1_hop_0_1_perc),\n",
    "        \"mean_edges_1_hop_0_2_perc\": np.mean(sizes_edge_1_hop_0_2_perc),\n",
    "        \"mean_edges_1_hop_0_5_perc\": np.mean(sizes_edge_1_hop_0_5_perc),\n",
    "        \"mean_edges_1_hop_1_perc\": np.mean(sizes_edge_1_hop_1_perc),\n",
    "        \"median_edges_1_hop_0_1_perc\": np.median(sizes_edge_1_hop_0_1_perc),\n",
    "        \"median_edges_1_hop_0_2_perc\": np.median(sizes_edge_1_hop_0_2_perc),\n",
    "        \"median_edges_1_hop_0_5_perc\": np.median(sizes_edge_1_hop_0_5_perc),\n",
    "        \"median_edges_1_hop_1_perc\": np.median(sizes_edge_1_hop_1_perc),\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results.to_parquet(\"results-rec-ver.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18b9e338-d4ff-4c68-b03f-d5c2175973c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_parquet(\"results-rec-ver.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1821b1-4fab-4947-be31-d73a007ad7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
