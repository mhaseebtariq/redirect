{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c949a975-6aac-497b-9680-8b708973d3db",
   "metadata": {},
   "source": [
    "### https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9762926"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd66cc69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "import uuid\n",
    "from collections import defaultdict\n",
    "from datetime import timedelta\n",
    "from glob import glob\n",
    "from itertools import product\n",
    "from pyspark.sql import functions as sf\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import igraph as ig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import settings as s\n",
    "from common import get_weights\n",
    "from communities import get_communities_spark\n",
    "from features import generate_features_spark, generate_features_udf_wrapper, SCHEMA_FEAT_UDF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78718f0-7f15-4c1f-963f-bd160efec0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (\n",
    "    sys.version_info.major,\n",
    "    sys.version_info.minor,\n",
    "    sys.version_info.micro,\n",
    ") != (3, 11, 8):\n",
    "    raise EnvironmentError(\n",
    "        \"Only runs efficiently on Python 3.11.8 (Tested on: Conda 24.1.2 | Apple M3 Pro)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "285f2aef-e6d2-48b2-b720-a02710e4e26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/20 13:24:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "SPARK_CONF = [\n",
    "    (\"spark.driver.memory\", \"32g\"),\n",
    "    (\"spark.worker.memory\", \"32g\"),\n",
    "    (\"spark.driver.maxResultSize\", \"32g\"),\n",
    "    (\"spark.driver.bindAddress\", \"127.0.0.1\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\"),\n",
    "    (\"spark.network.timeout\", \"600s\"),\n",
    "    (\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "]\n",
    "\n",
    "shutil.rmtree(\"artifacts\", ignore_errors=True)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"testing\")\n",
    "    .config(conf=SparkConf().setAll(SPARK_CONF))\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdfc52ce-1f9d-49d2-81f7-366fcdab6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_script = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "320b1fa7-53c8-4dbb-b25f-795aaca78a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_main = os.path.join(\"features\", \"libra\")\n",
    "# shutil.rmtree(location_main, ignore_errors=True)\n",
    "\n",
    "location_nodes_neighborhoods = f\"{location_main}{os.sep}nodes_neighborhoods.pickle\"\n",
    "\n",
    "location_comm_as_source_features = f\"{location_main}{os.sep}comm_as_source_features.parquet\"\n",
    "location_comm_as_target_features = f\"{location_main}{os.sep}comm_as_target_features.parquet\"\n",
    "location_comm_as_passthrough_features = f\"{location_main}{os.sep}comm_as_passthrough_features.parquet\"\n",
    "location_comm_as_passthrough_features_reverse = f\"{location_main}{os.sep}comm_as_passthrough_features_reverse.parquet\"\n",
    "\n",
    "location_features_node_level = f\"{location_main}{os.sep}features_node_level.parquet\"\n",
    "\n",
    "try:\n",
    "    os.makedirs(location_main)\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a2442c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/Libra_bank_3months_graph/data.csv\")\n",
    "rename = {\n",
    "    \"id_source\": \"source\",\n",
    "    \"id_destination\": \"target\",\n",
    "    \"cum_amount\": \"amount\",\n",
    "    \"nr_transactions\": \"num_transactions\",\n",
    "    \"nr_alerts\": \"alerts_count\",\n",
    "    \"nr_reports\": \"reports_count\",\n",
    "}\n",
    "data = data.rename(columns=rename)\n",
    "data.loc[:, \"source_\"] = (\n",
    "    data.loc[:, \"source\"].astype(str).apply(lambda x: f\"nid-{int(x)}\")\n",
    ")\n",
    "data.loc[:, \"target_\"] = (\n",
    "    data.loc[:, \"target\"].astype(str).apply(lambda x: f\"nid-{int(x)}\")\n",
    ")\n",
    "del data[\"source\"]\n",
    "del data[\"target\"]\n",
    "data = data.rename(\n",
    "    columns={\n",
    "        \"source_\": \"source\",\n",
    "        \"target_\": \"target\",\n",
    "    }\n",
    ").loc[:, [\"source\", \"target\", \"amount\", \"num_transactions\", \"alerts_count\", \"reports_count\"]]\n",
    "\n",
    "data = data.set_index([\"source\", \"target\"]).join(\n",
    "    get_weights(data).set_index([\"source\", \"target\"]), how=\"left\"\n",
    ").reset_index()\n",
    "data.loc[:, \"amount_weighted\"] = data.loc[:, \"amount\"] * data.loc[:, \"weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dab14662-5706-45b2-a53b-29b8b0b66760",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_data = pd.DataFrame(index=sorted(set(data[\"source\"].tolist() + data[\"target\"].tolist())))\n",
    "nodes_data.index.name = \"key\"\n",
    "\n",
    "w_alerts = int(data[\"alerts_count\"].sum() * 2)\n",
    "w_reports = int(data[\"reports_count\"].sum() * 2)\n",
    "\n",
    "w_alerts_source = data[data[\"alerts_count\"] > 0].groupby(\"source\").agg({\"alerts_count\": \"sum\"}).to_dict()[\"alerts_count\"]\n",
    "w_alerts_target = data[data[\"alerts_count\"] > 0].groupby(\"target\").agg({\"alerts_count\": \"sum\"}).to_dict()[\"alerts_count\"]\n",
    "\n",
    "w_reports_source = data[data[\"reports_count\"] > 0].groupby(\"source\").agg({\"reports_count\": \"sum\"}).to_dict()[\"reports_count\"]\n",
    "w_reports_target = data[data[\"reports_count\"] > 0].groupby(\"target\").agg({\"reports_count\": \"sum\"}).to_dict()[\"reports_count\"]\n",
    "\n",
    "nodes_data.loc[:, \"alert_weight\"] = nodes_data.index.map(\n",
    "    lambda x: (w_alerts_source.get(x, 0) + w_alerts_target.get(x, 0)) / w_alerts\n",
    ")\n",
    "nodes_data.loc[:, \"report_weight\"] = nodes_data.index.map(\n",
    "    lambda x: (w_reports_source.get(x, 0) + w_reports_target.get(x, 0)) / w_reports\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26195fe6-5b17-4525-bf16-83c63098e7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# data_input = spark.createDataFrame(data)\n",
    "# nodes_source = set(data[\"source\"].unique())\n",
    "# nodes_target = set(data[\"target\"].unique())\n",
    "# nodes_passthrough = nodes_source.intersection(nodes_target)\n",
    "\n",
    "# %run generate_flow_features.ipynb\n",
    "\n",
    "# comm_as_source_features.to_parquet(location_comm_as_source_features)\n",
    "# comm_as_target_features.to_parquet(location_comm_as_target_features)\n",
    "# comm_as_passthrough_features.to_parquet(location_comm_as_passthrough_features)\n",
    "# comm_as_passthrough_features_reverse.to_parquet(location_comm_as_passthrough_features_reverse)\n",
    "\n",
    "# del comm_as_source_features\n",
    "# del comm_as_target_features\n",
    "# del comm_as_passthrough_features\n",
    "# del comm_as_passthrough_features_reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf896f2-11d7-4f97-be4f-d4d6594c34da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing ego-net communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385100 4.0201 2.0 28.0 49596\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Constructing ego-net communities\")\n",
    "\n",
    "communities_ego = get_communities_spark(\n",
    "    [(x, [x]) for x in nodes_data.index], \n",
    "    ig.Graph.DataFrame(data.loc[:, [\"source\", \"target\"]], use_vids=False, directed=True), \n",
    "    os.cpu_count(), spark, 1, \"all\", 1e-100, None\n",
    ")\n",
    "sizes = [len(x[1]) for x in communities_ego]\n",
    "print(len(sizes), round(np.mean(sizes), 4), np.median(sizes), np.percentile(sizes, 99), np.max(sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e37941e-b6c3-478c-bd54-a16572719410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 1-hop communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385100 2.7259 2.0 15.0 312\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Constructing 1-hop communities\")\n",
    "\n",
    "communities_1_hop = get_communities_spark(\n",
    "    [(x, [x]) for x in nodes_data.index], \n",
    "    ig.Graph.DataFrame(data.loc[:, [\"source\", \"target\", \"amount_weighted\"]], use_vids=False, directed=True), \n",
    "    os.cpu_count(), spark, 1, \"all\", 0.01, \"amount_weighted\"\n",
    ")\n",
    "sizes = [len(x[1]) for x in communities_1_hop]\n",
    "print(len(sizes), round(np.mean(sizes), 4), np.median(sizes), np.percentile(sizes, 99), np.max(sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0a48ce1-9f64-4f58-8719-45bc6343d4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 2-hop-out communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385100 7.3424 3.0 44.0 1033\n",
      "CPU times: user 2.47 s, sys: 90.6 ms, total: 2.56 s\n",
      "Wall time: 34.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Constructing 2-hop-out communities\")\n",
    "\n",
    "communities_2_hop_out = get_communities_spark(\n",
    "    [(x, [x]) for x in nodes_data.index], \n",
    "    ig.Graph.DataFrame(data.loc[:, [\"source\", \"target\", \"amount_weighted\"]], use_vids=False, directed=True), \n",
    "    os.cpu_count(), spark, 2, \"out\", 0.01, \"amount_weighted\"\n",
    ")\n",
    "sizes = [len(x[1]) for x in communities_2_hop_out]\n",
    "print(len(sizes), round(np.mean(sizes), 4), np.median(sizes), np.percentile(sizes, 99), np.max(sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da7c20a1-7fb9-447c-969d-c1818143772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing 2-hop-in communities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385100 5.7006 3.0 33.0 772\n",
      "CPU times: user 2.98 s, sys: 86.6 ms, total: 3.06 s\n",
      "Wall time: 40 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Constructing 2-hop-in communities\")\n",
    "\n",
    "communities_2_hop_in = get_communities_spark(\n",
    "    [(x, [x]) for x in nodes_data.index], \n",
    "    ig.Graph.DataFrame(data.loc[:, [\"source\", \"target\", \"amount_weighted\"]], use_vids=False, directed=True), \n",
    "    os.cpu_count(), spark, 2, \"in\", 0.01, \"amount_weighted\"\n",
    ")\n",
    "sizes = [len(x[1]) for x in communities_2_hop_in]\n",
    "print(len(sizes), round(np.mean(sizes), 4), np.median(sizes), np.percentile(sizes, 99), np.max(sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17a5baa2-6ef2-461b-ba43-fc16bdfe6af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385100 12.3355 8.0 58.0 1066\n"
     ]
    }
   ],
   "source": [
    "communities_2_hop_in_dict = dict(communities_2_hop_in)\n",
    "communities_2_hop_combined = [\n",
    "    (x, y.union(communities_2_hop_in_dict[x])) for x, y in communities_2_hop_out\n",
    "]\n",
    "sizes = [len(x[1]) for x in communities_2_hop_combined]\n",
    "print(len(sizes), round(np.mean(sizes), 4), np.median(sizes), np.percentile(sizes, 99), np.max(sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4d20c9d-556c-407c-b4cb-c738a05b8994",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ig.Graph.DataFrame(data.loc[:, [\"source\", \"target\", \"amount\"]], use_vids=False, directed=True)\n",
    "nodes = [x[\"name\"] for x in graph.vs()]\n",
    "page_rank_d_uw = graph.personalized_pagerank(vertices=None, directed=True, weights=None)\n",
    "page_rank_d_uw = {k: v for k, v in zip(nodes, page_rank_d_uw)}\n",
    "page_rank_ud_uw = graph.personalized_pagerank(vertices=None, directed=False, weights=None)\n",
    "page_rank_ud_uw = {k: v for k, v in zip(nodes, page_rank_ud_uw)}\n",
    "page_rank_d_amounts = graph.personalized_pagerank(vertices=None, directed=True, weights=\"amount\")\n",
    "page_rank_d_amounts = {k: v for k, v in zip(nodes, page_rank_d_amounts)}\n",
    "page_rank_ud_amounts = graph.personalized_pagerank(vertices=None, directed=False, weights=\"amount\")\n",
    "page_rank_ud_amounts = {k: v for k, v in zip(nodes, page_rank_ud_amounts)}\n",
    "del graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d0d5614-c1d0-4b80-9dde-8c76c115177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = ig.Graph.DataFrame(data, use_vids=False, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb604f49-6cd7-45e8-863f-d132ce23ed75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 11s, sys: 20 s, total: 2min 31s\n",
      "Wall time: 5min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "features_ego = generate_features_spark(communities_ego, graph, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66b1390f-e28d-4def-ae4a-f57766bcf83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 11s, sys: 17.9 s, total: 2min 29s\n",
      "Wall time: 5min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "features_1_hop = generate_features_spark(communities_1_hop, graph, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "46eddf26-68a1-4cca-a526-bc8b7c63a69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 44s, sys: 18.5 s, total: 2min 2s\n",
      "Wall time: 4min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "features_2_hop_out = generate_features_spark(communities_2_hop_out, graph, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9210cf09-c6ae-4f48-8775-5d2f9fea2a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 47s, sys: 22.7 s, total: 2min 9s\n",
      "Wall time: 4min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "features_2_hop_in = generate_features_spark(communities_2_hop_in, graph, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ac3e778-42b0-49d6-9b68-2b6d123cd54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 23s, sys: 17.7 s, total: 2min 41s\n",
      "Wall time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "features_2_hop_combined = generate_features_spark(communities_2_hop_combined, graph, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6839964f-cb70-4f50-97be-96b0acdd8355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hop-source features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/20 14:49:31 WARN TaskSetManager: Stage 58 contains a task of very large size (6867 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.62 s, sys: 213 ms, total: 1.83 s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"1-hop-source features creation\")\n",
    "\n",
    "features_source = spark.createDataFrame(data).withColumn(\n",
    "    \"key\", sf.col(\"source\")\n",
    ").repartition(os.cpu_count(), \"key\").groupby(\"key\").applyInPandas(\n",
    "    generate_features_udf_wrapper(False), schema=SCHEMA_FEAT_UDF\n",
    ").toPandas()\n",
    "features_source = pd.DataFrame(features_source[\"features\"].apply(json.loads).tolist())\n",
    "features_source.columns = [f\"{s.G_1HOP_PREFIX}{x}\" if x != \"key\" else x for x in features_source.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb81ceb5-dcdd-4c9e-bb47-1429de489490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-hop-target features creation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/20 14:50:46 WARN TaskSetManager: Stage 61 contains a task of very large size (6961 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.62 s, sys: 203 ms, total: 1.83 s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"1-hop-target features creation\")\n",
    "\n",
    "features_target = spark.createDataFrame(data).withColumn(\n",
    "    \"key\", sf.col(\"target\")\n",
    ").repartition(os.cpu_count(), \"key\").groupby(\"key\").applyInPandas(\n",
    "    generate_features_udf_wrapper(False), schema=SCHEMA_FEAT_UDF\n",
    ").toPandas()\n",
    "features_target = pd.DataFrame(features_target[\"features\"].apply(json.loads).tolist())\n",
    "features_target.columns = [f\"{s.G_1HOP_PREFIX}{x}\" if x != \"key\" else x for x in features_target.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "74b28c47-b23c-44e2-b53e-a80b07eb971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ego.loc[:, \"page_rank_d_uw\"] = features_ego.loc[:, \"key\"].apply(lambda x: page_rank_d_uw[x])\n",
    "features_ego.loc[:, \"page_rank_ud_uw\"] = features_ego.loc[:, \"key\"].apply(lambda x: page_rank_ud_uw[x])\n",
    "features_ego.loc[:, \"page_rank_d_amounts\"] = features_ego.loc[:, \"key\"].apply(lambda x: page_rank_d_amounts[x])\n",
    "features_ego.loc[:, \"page_rank_ud_amounts\"] = features_ego.loc[:, \"key\"].apply(lambda x: page_rank_ud_amounts[x])\n",
    "\n",
    "features_1_hop\n",
    "features_2_hop_out\n",
    "features_2_hop_in\n",
    "features_2_hop_combined\n",
    "\n",
    "all_features = features_ego.set_index(\"key\").join(\n",
    "    features_1_hop.set_index(\"key\"), how=\"outer\", rsuffix=f\"_1_hop\"\n",
    ").join(\n",
    "    features_2_hop_out.set_index(\"key\"), how=\"outer\", rsuffix=f\"_2_hop_out\"\n",
    ").join(\n",
    "    features_2_hop_in.set_index(\"key\"), how=\"outer\", rsuffix=f\"_2_hop_in\"\n",
    ").join(\n",
    "    features_2_hop_combined.set_index(\"key\"), how=\"outer\", rsuffix=f\"_2_hop_combined\"\n",
    ").join(\n",
    "    features_source.set_index(\"key\"), how=\"outer\", rsuffix=f\"_as_source\"\n",
    ").join(\n",
    "    features_target.set_index(\"key\"), how=\"outer\", rsuffix=f\"_as_target\"\n",
    ")\n",
    "\n",
    "all_features = all_features.join(\n",
    "    pd.read_parquet(location_comm_as_source_features), how=\"left\", rsuffix=\"_dispense\"\n",
    ").join(\n",
    "    pd.read_parquet(location_comm_as_target_features), how=\"left\", rsuffix=\"_sink\"\n",
    ").join(\n",
    "    pd.read_parquet(location_comm_as_passthrough_features), how=\"left\", rsuffix=\"_passthrough\"\n",
    ").join(\n",
    "    pd.read_parquet(location_comm_as_passthrough_features_reverse), how=\"left\", rsuffix=\"_passthrough_rev\"\n",
    ")\n",
    "\n",
    "all_features.to_parquet(location_features_node_level)\n",
    "del all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85f3470e-c691-4aa7-b273-b4b0041f1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pd.read_parquet(location_features_node_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "09a76fe3-abb0-44b4-ac8a-b3b60eb55f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting graph_1_hop_feat_num_sources\n",
      "Deleting graph_1_hop_feat_num_source_and_target\n",
      "Deleting graph_1_hop_feat_num_source_only\n",
      "Deleting graph_1_hop_feat_std_debit_edges\n",
      "Deleting graph_1_hop_feat_std_debit_edges_weighted\n",
      "Deleting graph_1_hop_feat_num_targets_as_target\n",
      "Deleting graph_1_hop_feat_num_source_and_target_as_target\n",
      "Deleting graph_1_hop_feat_num_target_only_as_target\n",
      "Deleting graph_1_hop_feat_std_credit_edges_as_target\n",
      "Deleting graph_1_hop_feat_std_credit_edges_weighted_as_target\n"
     ]
    }
   ],
   "source": [
    "constants = []\n",
    "for column in all_features.columns:\n",
    "    if all_features[column].nunique(dropna=True) <= 1:\n",
    "        print(\"Deleting\", column)\n",
    "        del all_features[column]\n",
    "        constants.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1963559f-3817-41f4-8ea8-2869faf99e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians = {}\n",
    "for column in all_features.columns:\n",
    "    medians[column] = np.nanmedian(all_features[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8d6b4e1a-9296-4816-8f82-d0a1b9e0c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# outliers = all_features.loc[nodes_data[nodes_data[\"alert_weight\"] > 0].index, :]\n",
    "# rest = all_features.loc[nodes_data[nodes_data[\"alert_weight\"] == 0].index, :]\n",
    "\n",
    "# outliers_means = {}\n",
    "# for column in outliers.columns:\n",
    "#     scaled = MinMaxScaler(feature_range=(0, 1)).fit_transform(\n",
    "#         outliers[column].astype(np.float64).values.reshape(-1, 1)\n",
    "#     ).flatten()\n",
    "#     outliers_means[column] = np.nanmedian(scaled)\n",
    "\n",
    "# rest_means = {}\n",
    "# for column in rest.columns:\n",
    "#     scaled = MinMaxScaler(feature_range=(0, 1)).fit_transform(\n",
    "#         rest[column].astype(np.float64).values.reshape(-1, 1)\n",
    "#     ).flatten()\n",
    "#     rest_means[column] = np.nanmedian(scaled)\n",
    "\n",
    "# diffs = []\n",
    "# for column in all_features.columns:\n",
    "#     x, y = outliers_means[column], rest_means[column]\n",
    "#     x_y = (x+y) or 1\n",
    "#     diffs.append((column, abs(x-y) / x_y))\n",
    "# diffs = pd.DataFrame(diffs, columns=[\"feat\", \"diff\"])\n",
    "\n",
    "# selec_cols = diffs.sort_values(\"diff\").tail(100)[\"feat\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b6da3dca-7750-4520-9c78-4fa51c674804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 20s, sys: 22.5 s, total: 5min 43s\n",
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "anomalies = all_features.loc[:, []]\n",
    "model = IsolationForest(n_estimators=10_000, max_features=1.0)\n",
    "anomalies.loc[:, \"anomaly_score\"] = model.fit(\n",
    "    all_features.fillna(0)\n",
    ").decision_function(all_features.fillna(medians))\n",
    "anomalies = anomalies.sort_values(\"anomaly_score\", ascending=True)\n",
    "anomalies = anomalies.join(nodes_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "67d4d413-9b57-4fdd-a633-9d814c042afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385100 385 770 1926 3851\n"
     ]
    }
   ],
   "source": [
    "size = anomalies.shape[0]\n",
    "perc_point_1 = round(size * (0.1 / 100))\n",
    "perc_point_2 = round(size * (0.2 / 100))\n",
    "perc_point_5 = round(size * (0.5 / 100))\n",
    "perc_1 = round(size * (1 / 100))\n",
    "print(size, perc_point_1, perc_point_2, perc_point_5, perc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d8fbe31b-5c86-4bf0-aac0-f02144940b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_predicted_alert_weight(anomalies_input, perc_count):\n",
    "    anomalies_perc_x = anomalies_input.copy(deep=True)\n",
    "    index = anomalies_perc_x.head(perc_count).index.tolist()\n",
    "    anomalies_perc_x = anomalies_perc_x.loc[index, :]\n",
    "    anomalies_perc_x.loc[:, \"predicted_alert_weight\"] = anomalies_perc_x.loc[:, \"alert_weight\"]\n",
    "    anomalies_perc_x.loc[:, \"predicted_report_weight\"] = anomalies_perc_x.loc[:, \"report_weight\"]\n",
    "    return anomalies_perc_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d29c8983-063c-4fbe-8052-f1352de5772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomalies_perc_point_1 = add_predicted_alert_weight(anomalies, perc_point_1)\n",
    "anomalies_perc_point_2 = add_predicted_alert_weight(anomalies, perc_point_2)\n",
    "anomalies_perc_point_5 = add_predicted_alert_weight(anomalies, perc_point_5)\n",
    "anomalies_perc_1 = add_predicted_alert_weight(anomalies, perc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d249f063-be85-44c9-b1c7-b177f4dfd062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2776\n",
      "0.4197\n",
      "0.6412\n",
      "0.7553\n"
     ]
    }
   ],
   "source": [
    "print(round(anomalies_perc_point_1[\"predicted_alert_weight\"].sum(), 4))\n",
    "print(round(anomalies_perc_point_2[\"predicted_alert_weight\"].sum(), 4))\n",
    "print(round(anomalies_perc_point_5[\"predicted_alert_weight\"].sum(), 4))\n",
    "print(round(anomalies_perc_1[\"predicted_alert_weight\"].sum(), 4))\n",
    "# 0.2834\n",
    "# 0.4265\n",
    "# 0.6431\n",
    "# 0.7544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5a8070ee-4167-4977-afe6-74783c2ec1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.5711\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tpr = np.cumsum(anomalies_perc_1[\"predicted_alert_weight\"])\n",
    "print()\n",
    "print(round(np.mean(tpr), 4))\n",
    "print()\n",
    "# 0.5725"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b5e1a945-7c33-4409-9663-1787c1857a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2273\n",
      "0.4545\n",
      "0.6818\n",
      "0.9091\n"
     ]
    }
   ],
   "source": [
    "print(round(anomalies_perc_point_1[\"predicted_report_weight\"].sum(), 4))\n",
    "print(round(anomalies_perc_point_2[\"predicted_report_weight\"].sum(), 4))\n",
    "print(round(anomalies_perc_point_5[\"predicted_report_weight\"].sum(), 4))\n",
    "print(round(anomalies_perc_1[\"predicted_report_weight\"].sum(), 4))\n",
    "# 0.2273\n",
    "# 0.4545\n",
    "# 0.6818\n",
    "# 0.9091"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "13ed562b-97c2-4b23-9365-5a85bf506304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.6154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tpr = np.cumsum(anomalies_perc_1[\"predicted_report_weight\"])\n",
    "print()\n",
    "print(round(np.mean(tpr), 4))\n",
    "print()\n",
    "# 0.614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63156ca6-6a65-4712-9ce6-c0c933780ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = round(time.time() - start_script)\n",
    "print(f\"Script executed in {timedelta(seconds=delta)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9e338-d4ff-4c68-b03f-d5c2175973c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
